{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION AND METHODS LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z przegrzewacza\n",
    "data = pd.read_csv(\"..\\dane\\przegrzewacz\\oper_6-ACT-1-B_1-2.txt\", sep='\\t').reset_index()\n",
    "columns = list(data.columns)\n",
    "#There is no index column so we have to create one\n",
    "# Shift the labels\n",
    "columns = columns[1:] + [columns[0]]\n",
    "# Assign the new labels back to the DataFrame\n",
    "data.columns = columns\n",
    "# Drop the last column\n",
    "data = data.drop(data.columns[-1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['TS31P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Słownik z zakresami dla danych z każdego kolumuny (zakresy podane przez przemysł)\n",
    "ranges = {\n",
    "  't':(0, 0), 'B':(0, 100), 'FP4P':(30, 140), 'TS31P':(395, 445), 'GV3':(0, 100), 'TS32P':(385, 435), 'TP32P':(465, 515), 'GV4':(0, 100), 'TS42P':(455, 505),\n",
    "       'TP42P':(515, 565), 'SP31':(0, 0), 'CV31':(0, 100), 'SP32':(0, 0), 'CV32':(0, 100), 'SP41':(0, 0), 'CV41':(0, 100), 'SP42':(0, 0), 'CV42':(0, 100),\n",
    "       'FP3P':(30, 140), 'FP2P':(30, 140)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizacja danych na podstawie podanych zakresów->znormalizowane dane zostają zapisane do data_norm Dataframe\n",
    "data_norm = data.copy()\n",
    "for column in data_norm:\n",
    "  if(ranges[column] != (0, 0)): #jeśli jest podany zakres\n",
    "    data_norm[[column]] = (data_norm[[column]]-ranges[column][0])/(ranges[column][1]-ranges[column][0])\n",
    "  \n",
    "data_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling the data -> taking every 10th row\n",
    "data_resampled = data_norm.iloc[::10]\n",
    "data_resampled.reset_index(inplace= True, drop= True)\n",
    "\n",
    "data_resampled_raw = data.iloc[::10]\n",
    "data_resampled_raw.reset_index(inplace= True, drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za denormalizację wyników\n",
    "def denorm(data_norm, pred_name):\n",
    "  data_denorm = data_norm.copy()\n",
    "  for column in data_denorm:\n",
    "    if(ranges[pred_name] != (0, 0)): #jeśli jest podany zakres\n",
    "      data_denorm[[column]] = (data_denorm[[column]]*(ranges[pred_name][1]-ranges[pred_name][0]))+ranges[pred_name][0]\n",
    "  return data_denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wyodrębnienie zbiorów danych niezbędnych do stworzenia każdego z modeli\n",
    "#Model temperatury za schładzaczem 3\n",
    "df_TS32P_raw = data[['TS32P', 'TS31P', 'GV3']]\n",
    "df_TS32P = data_norm[['TS32P', 'TS31P', 'GV3']]\n",
    "df_TS32P_resampled = data_resampled[['TS32P', 'TS31P', 'GV3']]\n",
    "df_TS32P_resampled_raw = data_resampled_raw[['TS32P', 'TS31P', 'GV3']]\n",
    "\n",
    "#Model temperatury za przegrzewaczem 3\n",
    "df_TP32P_raw = data[['TP32P', 'TS32P', 'B', 'FP3P']]\n",
    "df_TP32P = data_norm[['TP32P', 'TS32P', 'B', 'FP3P']]\n",
    "df_TP32P_resampled = data_resampled[['TP32P', 'TS32P', 'B', 'FP3P']]\n",
    "df_TP32P_resampled_raw = data_resampled_raw[['TP32P', 'TS32P', 'B', 'FP3P']]\n",
    "\n",
    "#Model temperatury za schładzaczem 4\n",
    "df_TS42P_raw = data[['TS42P', 'GV4', 'FP3P','TP32P']]\n",
    "df_TS42P = data_norm[['TS42P', 'GV4', 'FP3P','TP32P']] #, 'TS41P'='TP32P'\n",
    "df_TS42P_resampled = data_resampled[['TS42P', 'GV4', 'FP3P','TP32P']]\n",
    "df_TS42P_resampled_raw = data_resampled_raw[['TS42P', 'GV4', 'FP3P','TP32P']]\n",
    "\n",
    "#Model temperatury za przegrzewaczem 4\n",
    "df_TP42P_raw = data[['TP42P', 'TS42P', 'B', 'FP4P']]\n",
    "df_TP42P = data_norm[['TP42P', 'TS42P', 'B', 'FP4P']]\n",
    "df_TP42P_resampled = data_resampled[['TP42P', 'TS42P', 'B', 'FP4P']]\n",
    "df_TP42P_resampled_raw = data_resampled_raw[['TP42P', 'TS42P', 'B', 'FP4P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja przetwarzająca szereg czasowy na próbki w postaci: window_size wartości poprzednich -> wartość następna\n",
    "def data_to_X_y(data, window_size=5):\n",
    "    data_as_np = data.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data_as_np)-window_size):\n",
    "        #Jako wejście bierzemy ciąg o długości WINDOW_SIZE-1\n",
    "        row = [[a] for a in data_as_np[i:i+window_size]]\n",
    "        X.append(row)\n",
    "        #Jako etykietę bierzemy następną wartość po ciągu\n",
    "        label = data_as_np[i+window_size]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za podział danych na zbiór treningowy, walidacyjny oraz testowy (przetwarzane są szeregi czasowe, więc nie mieszamy danych)\n",
    "def data_to_cross_val(sequences, variables, labels):\n",
    "  #Pobieramy rozmiar zbioru danych wejściowych\n",
    "  size = sequences.shape[0]\n",
    "  train = int(size*0.8)\n",
    "  val = int(size*0.9)\n",
    "  \n",
    "  sequences_train, variables_train, labels_train = sequences[:train], variables[:train], labels[:train]\n",
    "  sequences_val, variables_val, labels_val = sequences[train:val], variables[train:val], labels[train:val]\n",
    "  sequences_test, variables_test, labels_test = sequences[val:], variables[val:], labels[val:]\n",
    "  \n",
    "  return sequences_train, variables_train, labels_train, sequences_val, variables_val, labels_val, sequences_test, variables_test, labels_test\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras imports\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za tworzenie modelu\n",
    "def create_model(var_num):\n",
    "    # Define input layers\n",
    "    sequence_input = Input(shape=(None, 1))  #Values for the past time steps (input can have any window size)\n",
    "    variables_input = Input(shape=(var_num,))  # Variables for the current time step\n",
    "\n",
    "    # LSTM layers with dropout for sequences\n",
    "    lstm_sequence = LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(sequence_input)\n",
    "    lstm_sequence = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(lstm_sequence)\n",
    "\n",
    "    #Dense layer for additional variables at current time step\n",
    "    dense_variables = Dense(32, activation='relu')(variables_input)\n",
    "    dense_variables = Dropout(0.2)(dense_variables)\n",
    "    #scaled_variables = TrainableScaleLayer()(dense_variables)\n",
    "    \n",
    "    # Concatenate the outputs of both layers\n",
    "    merged = concatenate([lstm_sequence, dense_variables])\n",
    "    \n",
    "    # Dense layer for regression with linear activation function\n",
    "    output = Dense(1, activation='linear')(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[sequence_input, variables_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za przewidywanie kolejnych wartości -> wersja ze średnią ważoną\n",
    "def predict(model, input_sequence, input_variables, window_size, num_steps, labels):\n",
    "    predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "    sequence_input = np.array(input_sequence[0]) #We take only the first window_size values from the data\n",
    "    variables_input = np.array(input_variables) #We create a temporary variable for additional variables data\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "        next_value = model.predict([sequence_input.reshape(1, window_size, 1), variables_input[i].reshape(1,variables_input.shape[1])], verbose=0) \n",
    "        \n",
    "        #Adding predicted values to predictions array\n",
    "        predictions[i] = next_value[0][0]\n",
    "        \n",
    "        # Update the input sequence for the next iteration\n",
    "        sequence_input = np.roll(sequence_input, -1)  # Shift left one place\n",
    "        sequence_input[-1] = ((9 * next_value[0][0] + labels[i]) / 10)  # Insert the new value (weighted average) at the end\n",
    "        \n",
    "        #Counting how many steps forward have already been predicted and printing it to the console\n",
    "        if i % 500 == 0:\n",
    "            #os.system('cls')\n",
    "            print(i, '/', num_steps)\n",
    "    print(\"Done\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Funkcja odpowiedzialna za przewidywanie kolejnych wartości -> wersja bez średniej ważonej\n",
    "# def predict(model, input_sequence, input_variables, window_size, num_steps):\n",
    "#     predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "#     sequence_input = np.array(input_sequence[0]) #We take only the first window_size values from the data\n",
    "#     variables_input = np.array(input_variables) #We create a temporary variable for additional variables data\n",
    "    \n",
    "#     for i in range(num_steps):\n",
    "        \n",
    "#         #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "#         next_value = model.predict([sequence_input.reshape(1, window_size, 1), variables_input[i].reshape(1,variables_input.shape[1])], verbose=0) \n",
    "        \n",
    "#         #Adding predicted values to predictions array\n",
    "#         predictions[i] = next_value[0][0]\n",
    "        \n",
    "#         # Update the input sequence for the next iteration\n",
    "#         sequence_input = np.roll(sequence_input, -1)  # Shift left one place\n",
    "#         sequence_input[-1] = next_value[0][0]  # Insert the new value at the end\n",
    "        \n",
    "#         #Counting how many steps forward have already been predicted and printing it to the console\n",
    "#         if i % 500 == 0:\n",
    "#             #os.system('cls')\n",
    "#             print(i, '/', num_steps)\n",
    "#     print(\"Done\")\n",
    "#     return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowedzialna za wyświetlanie wyników\n",
    "def display(train_results, val_results, test_results, which_dataset, plot_name):\n",
    "    if which_dataset=='all':\n",
    "        # Create a figure and subplots for each dataset from cross validation\n",
    "        fig, axs = plt.subplots(\n",
    "            3, 2,               # 3 rows, 2 columns\n",
    "            figsize=(20, 16),    # Width = 10 , height = 8 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0, 0].plot(train_results['Train Predictions'], label='Model')\n",
    "        axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "        axs[0, 0].set_title('Training dataset')\n",
    "        axs[0, 0].set_xlabel('Time')\n",
    "        axs[0, 0].set_ylabel('Temperature')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[1, 0].plot(val_results['Val Predictions'], label='Model')\n",
    "        axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "        axs[1, 0].set_title('Validation dataset')\n",
    "        axs[1, 0].set_xlabel('Time')\n",
    "        axs[1, 0].set_ylabel('Temperature')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "        axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "        axs[2, 0].set_title('Testing dataset')\n",
    "        axs[2, 0].set_xlabel('Time')\n",
    "        axs[2, 0].set_ylabel('Temperature')\n",
    "        axs[2, 0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[0, 1].plot(train_results['Train Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "        axs[0, 1].set_title('Training dataset (Difference)')\n",
    "        axs[0, 1].set_xlabel('Time')\n",
    "        axs[0, 1].set_ylabel('Temperature')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 1].plot(val_results['Val Predictions']- val_results['Actuals'], label='Difference', color='red' )\n",
    "        axs[1, 1].set_title('Validation dataset (Difference)')\n",
    "        axs[1, 1].set_xlabel('Time')\n",
    "        axs[1, 1].set_ylabel('Temperature')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "        axs[2, 1].set_title('Testing dataset (Difference)')\n",
    "        axs[2, 1].set_xlabel('Time')\n",
    "        axs[2, 1].set_ylabel('Temperature')\n",
    "        axs[2, 1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "        \n",
    "    elif which_dataset=='train':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(train_results['Train Predictions'], label='Model')\n",
    "        axs[0].plot(train_results['Actuals'], label='Real')\n",
    "        axs[0].set_title('Training dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Temperature')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(train_results['Train Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "        axs[1].set_title('Training dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Temperature')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    elif which_dataset=='val':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(val_results['Val Predictions'], label='Model')\n",
    "        axs[0].plot(val_results['Actuals'], label='Real')\n",
    "        axs[0].set_title('Validation dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Temperature')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(val_results['Val Predictions'] - val_results['Actuals'], label='Difference', color='red')\n",
    "        axs[1].set_title('Validation dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Temperature')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "    \n",
    "    elif which_dataset=='test':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(test_results['Test Predictions'], label='Model')\n",
    "        axs[0].plot(test_results['Actuals'], label='Real')\n",
    "        axs[0].set_title('Testing dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Temperature')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "        axs[1].set_title('Testing dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Temperature')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    else: print(\"Please enter a valid dataset to display!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za wyświetlanie wyników z podanego przedziału\n",
    "def display_window(train_results, val_results, test_results, which_dataset, plot_name, train=0, val=0, test=0, window_train=3000, window_val=3000, window_test=3000):\n",
    "    if which_dataset=='all':\n",
    "        # Create a figure and subplots for each dataset from cross validation\n",
    "        fig, axs = plt.subplots(\n",
    "            3, 2,               # 3 rows, 2 columns\n",
    "            figsize=(20, 16),    # Width = 10 , height = 8 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0, 0].plot(train_results['Train Predictions'][train:train+window_train], label='Model') #\n",
    "        axs[0, 0].plot(train_results['Actuals'][train:train+window_train], label='Real')\n",
    "        axs[0, 0].set_title('Training dataset')\n",
    "        axs[0, 0].set_xlabel('Time')\n",
    "        axs[0, 0].set_ylabel('Temperature')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[1, 0].plot(val_results['Val Predictions'][val:val+window_val], label='Model') #\n",
    "        axs[1, 0].plot(val_results['Actuals'][val:val+window_val], label='Real')\n",
    "        axs[1, 0].set_title('Validation dataset')\n",
    "        axs[1, 0].set_xlabel('Time')\n",
    "        axs[1, 0].set_ylabel('Temperature')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[2, 0].plot(test_results['Test Predictions'][test:test+window_test], label='Model') #\n",
    "        axs[2, 0].plot(test_results['Actuals'][test:test+window_test], label='Real')\n",
    "        axs[2, 0].set_title('Testing dataset')\n",
    "        axs[2, 0].set_xlabel('Time')\n",
    "        axs[2, 0].set_ylabel('Temperature')\n",
    "        axs[2, 0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[0, 1].plot(train_results['Train Predictions'][train:train+window_train] - train_results['Actuals'][train:train+window_train], label='Difference', color='red') #\n",
    "        axs[0, 1].set_title('Training dataset (Difference)')\n",
    "        axs[0, 1].set_xlabel('Time')\n",
    "        axs[0, 1].set_ylabel('Temperature')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 1].plot(val_results['Val Predictions'][val:val+window_val]- val_results['Actuals'][val:val+window_val], label='Difference', color='red' ) #\n",
    "        axs[1, 1].set_title('Validation dataset (Difference)')\n",
    "        axs[1, 1].set_xlabel('Time')\n",
    "        axs[1, 1].set_ylabel('Temperature')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        axs[2, 1].plot(test_results['Test Predictions'][test:test+window_test] - test_results['Actuals'][test:test+window_test], label='Difference', color='red') #\n",
    "        axs[2, 1].set_title('Testing dataset (Difference)')\n",
    "        axs[2, 1].set_xlabel('Time')\n",
    "        axs[2, 1].set_ylabel('Temperature')\n",
    "        axs[2, 1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "        \n",
    "    elif which_dataset=='train':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(train_results['Train Predictions'][train:train+window_train], label='Model')\n",
    "        axs[0].plot(train_results['Actuals'][train:train+window_train], label='Real')\n",
    "        axs[0].set_title('Training dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Temperature')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(train_results['Train Predictions'][train:train+window_train] - train_results['Actuals'][train:train+window_train], label='Difference', color='red')\n",
    "        axs[1].set_title('Training dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Temperature')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    elif which_dataset=='val':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(val_results['Val Predictions'][val:val+window_val], label='Model')\n",
    "        axs[0].plot(val_results['Actuals'][val:val+window_val], label='Real')\n",
    "        axs[0].set_title('Validation dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Temperature')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(val_results['Val Predictions'][val:val+window_val] - val_results['Actuals'][val:val+window_val], label='Difference', color='red')\n",
    "        axs[1].set_title('Validation dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Temperature')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "    \n",
    "    elif which_dataset=='test':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(test_results['Test Predictions'][test:test+window_test], label='Model')\n",
    "        axs[0].plot(test_results['Actuals'][test:test+window_test], label='Real')\n",
    "        axs[0].set_title('Testing dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Temperature')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(test_results['Test Predictions'][test:test+window_test] - test_results['Actuals'][test:test+window_test], label='Difference', color='red')\n",
    "        axs[1].set_title('Testing dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Temperature')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    else: print(\"Please enter a valid dataset and parameters to display!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za zapisywanie dataframe do csv\n",
    "def save(df, file_name):\n",
    "  df.to_csv('predictions_new/'+file_name+'.csv')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za odczytywanie dataframe z csv\n",
    "def get(file_name):\n",
    "  return pd.read_csv('predictions_new/'+file_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porównanie wykresów trzech różnych modeli na zbiorze testowym + jednostki\n",
    "\n",
    "def display_comparison(train_results, val_results, test_results, plot_name, y_unit='°C', x_unit='10s'):\n",
    "    # Create a figure and subplots for each dataset from cross validation\n",
    "    fig, axs = plt.subplots(\n",
    "        3, 2,               \n",
    "        figsize=(20, 16),    \n",
    "        gridspec_kw=dict(hspace=0.5, wspace=0.5)   \n",
    "    )\n",
    "\n",
    "    # Plot data on subplots\n",
    "    axs[0, 0].plot(train_results['Test Predictions'], label='Model')\n",
    "    axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "    axs[0, 0].set_title('Testing dataset LSTM')\n",
    "    axs[0, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 0].set_ylabel(f'Temperature [{y_unit}]')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[1, 0].plot(val_results['Test Predictions'], label='Model')\n",
    "    axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "    axs[1, 0].set_title('Testing dataset MLP ARX')\n",
    "    axs[1, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 0].set_ylabel(f'Temperature [{y_unit}]')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "    axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "    axs[2, 0].set_title('Testing dataset LINEAR ARX')\n",
    "    axs[2, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 0].set_ylabel(f'Temperature [{y_unit}]')\n",
    "    axs[2, 0].legend()\n",
    "\n",
    "    # Plot difference between model and real data\n",
    "    axs[0, 1].plot(train_results['Test Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "    axs[0, 1].set_title('Testing dataset LSTM (Difference)')\n",
    "    axs[0, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 1].plot(val_results['Test Predictions'] - val_results['Actuals'], label='Difference', color='red')\n",
    "    axs[1, 1].set_title('Testing dataset MLP ARX (Difference)')\n",
    "    axs[1, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "    axs[2, 1].set_title('Testing dataset LINEAR ARX (Difference)')\n",
    "    axs[2, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[2, 1].legend()\n",
    "\n",
    "    fig.suptitle(plot_name, fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porównanie wykresów wyników jednego modelu (train, val, test) + jednostki\n",
    "\n",
    "def display_train_val_test(train_results, val_results, test_results, plot_name, y_unit='°C', x_unit='10s'):\n",
    "    # Create a figure and subplots for each dataset from cross validation\n",
    "    fig, axs = plt.subplots(\n",
    "        3, 2,               \n",
    "        figsize=(20, 16),    \n",
    "        gridspec_kw=dict(hspace=0.5, wspace=0.5)\n",
    "    )\n",
    "\n",
    "    # Plot data on subplots\n",
    "    axs[0, 0].plot(train_results['Train Predictions'], label='Model')\n",
    "    axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "    axs[0, 0].set_title('Training dataset')\n",
    "    axs[0, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 0].set_ylabel(f'Temperature [{y_unit}]')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[1, 0].plot(val_results['Val Predictions'], label='Model')\n",
    "    axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "    axs[1, 0].set_title('Validation dataset')\n",
    "    axs[1, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 0].set_ylabel(f'Temperature [{y_unit}]')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "    axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "    axs[2, 0].set_title('Testing dataset')\n",
    "    axs[2, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 0].set_ylabel(f'Temperature [{y_unit}]')\n",
    "    axs[2, 0].legend()\n",
    "\n",
    "    # Plot difference between model and real data\n",
    "    axs[0, 1].plot(train_results['Train Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "    axs[0, 1].set_title('Training dataset (Difference)')\n",
    "    axs[0, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 1].plot(val_results['Val Predictions'] - val_results['Actuals'], label='Difference', color='red')\n",
    "    axs[1, 1].set_title('Validation dataset (Difference)')\n",
    "    axs[1, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "    axs[2, 1].set_title('Testing dataset (Difference)')\n",
    "    axs[2, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[2, 1].legend()\n",
    "\n",
    "    fig.suptitle(plot_name, fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za wyświetlanie wyników z testing dataset\n",
    "def display_test_results(test_results, plot_name):\n",
    "    # Create a figure and subplots for the test dataset\n",
    "    fig, axs = plt.subplots(\n",
    "        2, 1,               # 2 rows, 1 column\n",
    "        figsize=(20, 10),    # Width = 20, height = 10\n",
    "        gridspec_kw=dict(hspace=0.5)   # Vertical spacing between subplots\n",
    "    )\n",
    "\n",
    "    # Plot predictions vs actuals\n",
    "    axs[0].plot(test_results['Test Predictions'], label='Model Predictions')\n",
    "    axs[0].plot(test_results['Actuals'], label='Actual Values')\n",
    "    axs[0].set_title('Testing Dataset Predictions vs Actuals')\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_ylabel('Value')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot the difference between predictions and actuals\n",
    "    axs[1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Prediction Error', color='red')\n",
    "    axs[1].set_title('Testing Dataset Prediction Error')\n",
    "    axs[1].set_xlabel('Time')\n",
    "    axs[1].set_ylabel('Value')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set the main title and show the plot\n",
    "    fig.suptitle(plot_name, fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    me = (y_true - y_pred).mean()\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    mae = (y_true - y_pred).abs().mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    return me, mse, mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION AND METHODS MLP ARX AND LINEAR ARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling and adding shifted output values as inputs\n",
    "#Model temperatury za schładzaczem 3\n",
    "df_TS32P_MLP = df_TS32P_resampled\n",
    "df_TS32P_MLP['y_k_1'] = df_TS32P_MLP['TS32P'].shift(1)  # y[k-1]\n",
    "df_TS32P_MLP['y_k_2'] = df_TS32P_MLP['TS32P'].shift(2)  # y[k-2]\n",
    "df_TS32P_MLP = df_TS32P_MLP.dropna().reset_index(drop=True)\n",
    "\n",
    "#Model temperatury za przegrzewaczem 3\n",
    "df_TP32P_MLP = df_TP32P_resampled\n",
    "df_TP32P_MLP['y_k_1'] = df_TP32P_MLP['TP32P'].shift(1)  # y[k-1]\n",
    "df_TP32P_MLP['y_k_2'] = df_TP32P_MLP['TP32P'].shift(2)  # y[k-2]\n",
    "df_TP32P_MLP = df_TP32P_MLP.dropna().reset_index(drop=True)\n",
    "\n",
    "#Model temperatury za schładzaczem 4\n",
    "df_TS42P_MLP = df_TS42P_resampled\n",
    "df_TS42P_MLP['y_k_1'] = df_TS42P_MLP['TS42P'].shift(1)  # y[k-1]\n",
    "df_TS42P_MLP['y_k_2'] = df_TS42P_MLP['TS42P'].shift(2)  # y[k-2]\n",
    "df_TS42P_MLP = df_TS42P_MLP.dropna().reset_index(drop=True)\n",
    "\n",
    "#Model temperatury za przegrzewaczem 4\n",
    "df_TP42P_MLP = df_TP42P_resampled\n",
    "df_TP42P_MLP['y_k_1'] = df_TP42P_MLP['TP42P'].shift(1)  # y[k-1]\n",
    "df_TP42P_MLP['y_k_2'] = df_TP42P_MLP['TP42P'].shift(2)  # y[k-2]\n",
    "df_TP42P_MLP = df_TP42P_MLP.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za tworzenie modelu MLP\n",
    "def create_model_MLP(var_num):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation = 'relu', input_dim = var_num))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za tworzenie modelu MLP with one layer and linear activation function\n",
    "def create_model_MLP_onelayer(var_num):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation = 'linear', input_dim = var_num))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za przewidywanie kolejnych wartości MLP ARX-> wersja ze średnią ważoną\n",
    "def predict_MLP(model, input, num_steps, labels):\n",
    "    predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "    input = np.array(input) #We create a temporary variable for input\n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "        next_value = model.predict(input[i].reshape(1,input.shape[1]), verbose=0) \n",
    "        \n",
    "        #Adding predicted values to predictions array\n",
    "        predictions[i] = next_value[0][0]\n",
    "        \n",
    "        #Switching y[k-1] and y[k-2] values for those predicted by the model\n",
    "        if i<num_steps-1:\n",
    "           input[i+1][input.shape[1]-2] = ((9 * predictions[i] + labels[i]) / 10) #y[k-1] one before last column\n",
    "           if i!=0:\n",
    "               input[i+1][input.shape[1]-1] = ((9 * predictions[i-1] + labels[i-1]) / 10)#y[k-2] last column\n",
    "           else:\n",
    "               pass\n",
    "        \n",
    "        \n",
    "        #Counting how many steps forward have already been predicted and printing it to the console\n",
    "        if i % 500 == 0:\n",
    "            #os.system('cls')\n",
    "            print(i, '/', num_steps)\n",
    "    print(\"Done\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Funkcja odpowiedzialna za przewidywanie kolejnych wartości MLP ARX-> wersja bez średniej ważonej\n",
    "# def predict_MLP(model, input, num_steps):\n",
    "#     predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "#     input = np.array(input) #We create a temporary variable for input\n",
    "    \n",
    "#     for i in range(num_steps):\n",
    "        \n",
    "#         #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "#         next_value = model.predict(input[i].reshape(1,input.shape[1]), verbose=0) \n",
    "        \n",
    "#         #Adding predicted values to predictions array\n",
    "#         predictions[i] = next_value[0][0]\n",
    "        \n",
    "#         #Switching y[k-1] and y[k-2] values for those predicted by the model\n",
    "#         if i<num_steps-1:\n",
    "#            input[i+1][input.shape[1]-2] = predictions[i]#y[k-1] one before last column\n",
    "#            if i!=0:\n",
    "#                input[i+1][input.shape[1]-1] = predictions[i-1]#y[k-2] last column\n",
    "#            else:\n",
    "#                pass\n",
    "        \n",
    "#         #Counting how many steps forward have already been predicted and printing it to the console\n",
    "#         if i % 500 == 0:\n",
    "#             #os.system('cls')\n",
    "#             print(i, '/', num_steps)\n",
    "#     print(\"Done\")\n",
    "#     return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za przewidywanie kolejnych wartości LINEAR ARX-> wersja ze średnią ważoną\n",
    "def predict_LINEAR(model, input, num_steps, labels):\n",
    "    predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "    input = np.array(input) #We create a temporary variable for input\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "        next_value = model.predict(input[i].reshape(1,input.shape[1])) \n",
    "        \n",
    "        #Adding predicted values to predictions array\n",
    "        predictions[i] = next_value[0]\n",
    "        \n",
    "        #Switching y[k-1] and y[k-2] values for those predicted by the model\n",
    "    \n",
    "        if i<num_steps-1: \n",
    "           input[i+1][input.shape[1]-2] = ((9 * predictions[i] + labels[i]) / 10) #y[k-1] one before last column\n",
    "           if i!=0:\n",
    "               input[i+1][input.shape[1]-1] = ((9 * predictions[i-1] + labels[i-1]) / 10)#y[k-2] last column\n",
    "           else:\n",
    "               pass\n",
    "        \n",
    "        #Counting how many steps forward have already been predicted and printing it to the console\n",
    "        if i % 500 == 0:\n",
    "            #os.system('cls')\n",
    "            print(i, '/', num_steps)\n",
    "    print(\"Done\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns for dataframe for each model as python lists\n",
    "#Wyodrębnienie zbiorów danych niezbędnych do stworzenia każdego z modeli\n",
    "\n",
    "#Model temperatury za schładzaczem 3\n",
    "col_TS32P = ['TS32P', 'TS31P', 'GV3']\n",
    "\n",
    "#Model temperatury za przegrzewaczem 3\n",
    "col_TP32P= ['TP32P', 'TS32P', 'B', 'FP3P']\n",
    "\n",
    "#Model temperatury za schładzaczem 4\n",
    "col_TS42P = ['TS42P', 'GV4', 'FP3P','TP32P']\n",
    "\n",
    "#Model temperatury za przegrzewaczem 4\n",
    "col_TP42P = ['TP42P', 'TS42P', 'B', 'FP4P']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_TS32P_LSTM'), get('test_results_TS32P_MLP'), get('test_results_TS32P_LINEAR'), \"MODEL 1 - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TS32P['TS32P'], WINDOW_SIZE)\n",
    "var = df_TS32P[['TS31P','GV3']][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model1 = create_model(2) #2 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_TS32P/model_TS32P.keras', save_best_only=True)\n",
    "\n",
    "model1.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model1.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model1 = load_model('model_TS32P/model_TS32P.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "# train_predictions = predict(model1, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_TS32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_TS32P, 'TS32P'),'train_results_TS32P')\n",
    "# save(train_results_TS32P,'train_results_TS32P_normalized')\n",
    "\n",
    "# val_predictions = predict(model1, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_TS32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_TS32P, 'TS32P'),'val_results_TS32P')\n",
    "# save(val_results_TS32P,'val_results_TS32P_normalized')\n",
    "\n",
    "test_predictions = predict(model1, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TS32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS32P, 'TS32P'), 'test_results_TS32P')\n",
    "save(test_results_TS32P,'test_results_TS32P_normalized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display(get('train_results_TS32P'), get('val_results_TS32P'), get('test_results_TS32P'),'test', \"MODEL 1 - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3) LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TS32P_resampled['TS32P'], WINDOW_SIZE)\n",
    "var = df_TS32P_resampled[['TS31P','GV3']][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model1 = create_model(2) #2 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_TS32P/model_TS32P_LSTM_new.keras', save_best_only=True)\n",
    "\n",
    "model1.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model1.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=50, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model1 = load_model('model_TS32P/model_TS32P_LSTM.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict(model1, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "train_results_TS32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TS32P, 'TS32P'),'train_results_TS32P_LSTM')\n",
    "save(train_results_TS32P,'train_results_TS32P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TS32P[\"Actuals\"], train_results_TS32P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict(model1, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_TS32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TS32P, 'TS32P'),'val_results_TS32P_LSTM')\n",
    "save(val_results_TS32P,'val_results_TS32P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TS32P[\"Actuals\"], val_results_TS32P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict(model1, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TS32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS32P, 'TS32P'), 'test_results_TS32P_LSTM')\n",
    "save(test_results_TS32P,'test_results_TS32P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TS32P[\"Actuals\"], test_results_TS32P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TS32P[\"Actuals\"], test_results_TS32P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "#display_test_results(get('test_results_TS32P_LSTM'), \"MODEL 1 - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3) LSTM\")\n",
    "display_train_val_test(get('train_results_TS32P_LSTM'), get('val_results_TS32P_LSTM'), get('test_results_TS32P_LSTM'), \"MODEL 1 LSTM - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "size = df_TS32P_MLP.shape[0]\n",
    "train = int(size*0.8)\n",
    "val = int(size*0.9)\n",
    "\n",
    "X_train = df_TS32P_MLP[col_TS32P[1:] + ['y_k_1','y_k_2']][:train]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = df_TS32P_MLP[col_TS32P[0]][:train]\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "X_val = df_TS32P_MLP[col_TS32P[1:] + ['y_k_1','y_k_2']][train:val]\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = df_TS32P_MLP[col_TS32P[0]][train:val]\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "X_test = df_TS32P_MLP[col_TS32P[1:] + ['y_k_1','y_k_2']][val:]\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = df_TS32P_MLP[col_TS32P[0]][val:]\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation MLP\n",
    "model1 = create_model_MLP(4) #2+2y\n",
    "\n",
    "cp = ModelCheckpoint('model_TS32P/model_TS32P_MLP_new.keras', save_best_only=True)\n",
    "\n",
    "model1.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training MLP\n",
    "model1.fit(X_train, y_train, validation_data=(X_val,  y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions MLP\n",
    "model1 = load_model('model_TS32P/model_TS32P_MLP.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_MLP(model1, X_train, window_train, y_train)\n",
    "train_results_TS32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TS32P, 'TS32P'),'train_results_TS32P_MLP')\n",
    "save(train_results_TS32P,'train_results_TS32P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TS32P[\"Actuals\"], train_results_TS32P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_MLP(model1, X_val, window_val, y_val)\n",
    "val_results_TS32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TS32P, 'TS32P'),'val_results_TS32P_MLP')\n",
    "save(val_results_TS32P,'val_results_TS32P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TS32P[\"Actuals\"], val_results_TS32P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_MLP(model1, X_test, window_test, y_test)\n",
    "test_results_TS32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS32P, 'TS32P'), 'test_results_TS32P_MLP')\n",
    "save(test_results_TS32P,'test_results_TS32P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TS32P[\"Actuals\"], test_results_TS32P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me, mse, mae, rmse = calculate_metrics(test_results_TS32P[\"Actuals\"], test_results_TS32P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model1 = LinearRegression()\n",
    "#Model training linear\n",
    "model1.fit(X_train, y_train)\n",
    "#model1 = load_model('model_TS32P/model_TS32P_LINEAR.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_LINEAR(model1, X_train, window_train, y_train)\n",
    "train_results_TS32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TS32P, 'TS32P'),'train_results_TS32P_LINEAR')\n",
    "save(train_results_TS32P,'train_results_TS32P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TS32P[\"Actuals\"], train_results_TS32P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_LINEAR(model1, X_val, window_val, y_val)\n",
    "val_results_TS32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TS32P, 'TS32P'),'val_results_TS32P_LINEAR')\n",
    "save(val_results_TS32P,'val_results_TS32P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TS32P[\"Actuals\"], val_results_TS32P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_LINEAR(model1, X_test, window_test, y_test)\n",
    "test_results_TS32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS32P, 'TS32P'), 'test_results_TS32P_LINEAR')\n",
    "save(test_results_TS32P,'test_results_TS32P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TS32P[\"Actuals\"], test_results_TS32P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TS32P[\"Actuals\"], test_results_TS32P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_TS32P_MLP'), \"MODEL 1 - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3) MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TS32P_MLP'), get('val_results_TS32P_MLP'), get('test_results_TS32P_MLP'), \"MODEL 1 MLP ARX - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_TS32P_LINEAR'), \"MODEL 1 - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3) LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TS32P_LINEAR'), get('val_results_TS32P_LINEAR'), get('test_results_TS32P_LINEAR'), \"MODEL 1 LINEAR ARX - Model temperatury za schładzaczem 3 - TS32P (TS31P, GV3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 1 - Model temperatury za schładzaczem 3 - TS32P', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/TS32P_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min_green(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.style.apply(highlight_min_green, subset=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/TS32P_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_TP32P_LSTM'), get('test_results_TP32P_MLP'), get('test_results_TP32P_LINEAR'), \"MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P (TP32P, B, FP3P)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TP32P['TP32P'], WINDOW_SIZE)\n",
    "#df_TP32P['TS32P'] = df_TP32P['TS32P'].shift(-5).fillna(method = 'bfill')\n",
    "var = df_TP32P[['TS32P', 'B', 'FP3P']][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model2 = create_model(3) #3 additional variables\n",
    "\n",
    "cp = ModelCheckpoint('model_TP32P/model_TP32P.keras', save_best_only=True)\n",
    "\n",
    "model2.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model2 = load_model('model_TP32P/model_TP32P.keras')\n",
    "model2.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model2 = load_model('model_TP32P/model_TP32P.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "# train_predictions = predict(model2, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_TP32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_TP32P,'TP32P'),'train_results_TP32P')\n",
    "# save(train_results_TP32P, 'train_results_TP32P_normalized')\n",
    "\n",
    "# val_predictions = predict(model2, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_TP32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_TP32P, 'TP32P'),'val_results_TP32P')\n",
    "# save(val_results_TP32P, 'val_results_TP32P_normalized')\n",
    "\n",
    "test_predictions = predict(model2, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TP32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP32P, 'TP32P'), 'test_results_TP32P')\n",
    "save(test_results_TP32P, 'test_results_TP32P_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display(get('train_results_TP32P'), get('val_results_TP32P'), get('test_results_TP32P_raw_data'),'test', \"MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P) LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TP32P_resampled['TP32P'], WINDOW_SIZE)\n",
    "#df_TP32P['TS32P'] = df_TP32P['TS32P'].shift(-5).fillna(method = 'bfill')\n",
    "var = df_TP32P_resampled[['TS32P', 'B', 'FP3P']][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model2 = create_model(3) #3 additional variables\n",
    "\n",
    "cp = ModelCheckpoint('model_TP32P/model_TP32P_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model2.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model2 = load_model('model_TP32P/model_TP32P_LSTM.keras')\n",
    "model2.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=20, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model2 = load_model('model_TP32P/model_TP32P_LSTM.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict(model2, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "train_results_TP32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TP32P, 'TP32P'),'train_results_TP32P_LSTM')\n",
    "save(train_results_TP32P,'train_results_TP32P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TP32P[\"Actuals\"], train_results_TP32P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict(model2, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_TP32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TP32P, 'TP32P'),'val_results_TP32P_LSTM')\n",
    "save(val_results_TP32P,'val_results_TP32P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TP32P[\"Actuals\"], val_results_TP32P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict(model2, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TP32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP32P, 'TP32P'), 'test_results_TP32P_LSTM')\n",
    "save(test_results_TP32P,'test_results_TP32P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TP32P[\"Actuals\"], test_results_TP32P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TP32P[\"Actuals\"], test_results_TP32P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_TP32P_LSTM'), \"MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P) LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TP32P_LSTM'), get('val_results_TP32P_LSTM'), get('test_results_TP32P_LSTM'), \"MODEL 2 LSTM - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "size = df_TP32P_MLP.shape[0]\n",
    "train = int(size*0.8)\n",
    "val = int(size*0.9)\n",
    "\n",
    "X_train = df_TP32P_MLP[col_TP32P[1:] + ['y_k_1','y_k_2']][:train]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = df_TP32P_MLP[col_TP32P[0]][:train]\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "X_val = df_TP32P_MLP[col_TP32P[1:] + ['y_k_1','y_k_2']][train:val]\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = df_TP32P_MLP[col_TP32P[0]][train:val]\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "X_test = df_TP32P_MLP[col_TP32P[1:] + ['y_k_1','y_k_2']][val:]\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = df_TP32P_MLP[col_TP32P[0]][val:]\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model2 = create_model_MLP(5) #3+2y\n",
    "\n",
    "cp = ModelCheckpoint('model_TP32P/model_TP32P_MLP.keras', save_best_only=True)\n",
    "\n",
    "model2.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model2 = load_model('model_TP32P/model_TP32P_MLP.keras')\n",
    "model2.fit(X_train, y_train, validation_data=(X_val,  y_val), epochs=20, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions MLP\n",
    "model2 = load_model('model_TP32P/model_TP32P_MLP.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_MLP(model2, X_train, window_train, y_train)\n",
    "train_results_TP32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TP32P, 'TP32P'),'train_results_TP32P_MLP')\n",
    "save(train_results_TP32P,'train_results_TP32P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TP32P[\"Actuals\"], train_results_TP32P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_MLP(model2, X_val, window_val, y_val)\n",
    "val_results_TP32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TP32P, 'TP32P'),'val_results_TP32P_MLP')\n",
    "save(val_results_TP32P,'val_results_TP32P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TP32P[\"Actuals\"], val_results_TP32P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_MLP(model2, X_test, window_test, y_test)\n",
    "test_results_TP32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP32P, 'TP32P'), 'test_results_TP32P_MLP')\n",
    "save(test_results_TP32P,'test_results_TP32P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TP32P[\"Actuals\"], test_results_TP32P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TP32P[\"Actuals\"], test_results_TP32P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model2 = LinearRegression()\n",
    "#Model training linear\n",
    "model2.fit(X_train, y_train)\n",
    "#model2 = load_model('model_TP32P/model_TP32P_LINEAR.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_LINEAR(model2, X_train, window_train, y_train)\n",
    "train_results_TP32P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TP32P, 'TP32P'),'train_results_TP32P_LINEAR')\n",
    "save(train_results_TP32P,'train_results_TP32P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TP32P[\"Actuals\"], train_results_TP32P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_LINEAR(model2, X_val, window_val, y_val)\n",
    "val_results_TP32P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TP32P, 'TP32P'),'val_results_TP32P_LINEAR')\n",
    "save(val_results_TP32P,'val_results_TP32P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TP32P[\"Actuals\"], val_results_TP32P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_LINEAR(model2, X_test, window_test, y_test)\n",
    "test_results_TP32P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP32P, 'TP32P'), 'test_results_TP32P_LINEAR')\n",
    "save(test_results_TP32P,'test_results_TP32P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TP32P[\"Actuals\"], test_results_TP32P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TP32P[\"Actuals\"], test_results_TP32P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_TP32P_MLP'), \"MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P) MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TP32P_MLP'), get('val_results_TP32P_MLP'), get('test_results_TP32P_MLP'), \"MODEL 2 MLP ARX - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_TP32P_LINEAR'), \"MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P) LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TP32P_LINEAR'), get('val_results_TP32P_LINEAR'), get('test_results_TP32P_LINEAR'), \"MODEL 2 LINEAR ARX - Model temperatury za przegrzewaczem 3 - TP32P (TS32P, B, FP3P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 2 - Model temperatury za przegrzewaczem 3 - TP32P', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/TP32P_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/TP32P_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 3 - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_TS42P_LSTM_test'), get('test_results_TS42P_MLP'), get('test_results_TS42P_LINEAR'), \"MODEL 3 - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TS42P['TS42P'], WINDOW_SIZE)\n",
    "var = df_TS42P[['GV4', 'FP3P', 'TP32P']][WINDOW_SIZE-1:].to_numpy()  #, 'TS41P'='TS32P'\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model3 = create_model(3) #3 additional variables ('TS41P'='TS32P')\n",
    "\n",
    "cp = ModelCheckpoint('model_TS42P/model_TS42P.keras', save_best_only=True)\n",
    "\n",
    "model3.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model3.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model3 = load_model('model_TS42P/model_TS42P.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "# train_predictions = predict(model3, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_TS42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(train_results_TS42P,'train_results_TS42P_normalized')\n",
    "# save(denorm(train_results_TS42P,'TS42P'),'train_results_TS42P')\n",
    "\n",
    "# val_predictions = predict(model3, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_TS42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(val_results_TS42P,'val_results_TS42P_normalized')\n",
    "# save(denorm(val_results_TS42P,'TS42P'),'val_results_TS42P')\n",
    "\n",
    "test_predictions = predict(model3, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TS42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(test_results_TS42P, 'test_results_TS42P_normalized')\n",
    "save(denorm(test_results_TS42P,'TS42P'), 'test_results_TS42P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display(get('train_results_TS42P'), get('val_results_TS42P'), get('test_results_TS42P'),'test', \"MODEL 3 - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TS42P_resampled['TS42P'], WINDOW_SIZE)\n",
    "var = df_TS42P_resampled[['GV4', 'FP3P', 'TP32P']][WINDOW_SIZE-1:].to_numpy()  #, 'TS41P'='TS32P'\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model3 = create_model(3) #3 additional variables ('TS41P'='TS32P')\n",
    "\n",
    "cp = ModelCheckpoint('model_TS42P/model_TS42P_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model3.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model3 = load_model('model_TS42P/model_TS42P_LSTM.keras')\n",
    "model3.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model3 = load_model('model_TS42P/model_TS42P_LSTM.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict(model3, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "train_results_TS42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TS42P, 'TS42P'),'train_results_TS42P_LSTM')\n",
    "save(train_results_TS42P,'train_results_TS42P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TS42P[\"Actuals\"], train_results_TS42P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict(model3, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_TS42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TS42P, 'TS42P'),'val_results_TS42P_LSTM')\n",
    "save(val_results_TS42P,'val_results_TS42P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TS42P[\"Actuals\"], val_results_TS42P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict(model3, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TS42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS42P, 'TS42P'), 'test_results_TS42P_LSTM')\n",
    "save(test_results_TS42P,'test_results_TS42P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TS42P[\"Actuals\"], test_results_TS42P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TS42P[\"Actuals\"], test_results_TS42P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results( get('test_results_TS42P_LSTM'), \"MODEL 3 - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P) LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TS42P_LSTM'), get('val_results_TS42P_LSTM'), get('test_results_TS42P_LSTM'), \"MODEL 3 LSTM - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "size = df_TS42P_MLP.shape[0]\n",
    "train = int(size*0.8)\n",
    "val = int(size*0.9)\n",
    "\n",
    "X_train = df_TS42P_MLP[col_TS42P[1:] + ['y_k_1','y_k_2']][:train]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = df_TS42P_MLP[col_TS42P[0]][:train]\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "X_val = df_TS42P_MLP[col_TS42P[1:] + ['y_k_1','y_k_2']][train:val]\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = df_TS42P_MLP[col_TS42P[0]][train:val]\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "X_test = df_TS42P_MLP[col_TS42P[1:] + ['y_k_1','y_k_2']][val:]\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = df_TS42P_MLP[col_TS42P[0]][val:]\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model3 = create_model_MLP(5) #3+2y\n",
    "\n",
    "cp = ModelCheckpoint('model_TS42P/model_TS42P_MLP.keras', save_best_only=True)\n",
    "\n",
    "model3.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model3.fit(X_train, y_train, validation_data=(X_val,  y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions MLP\n",
    "model3 = load_model('model_TS42P/model_TS42P_MLP.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_MLP(model3, X_train, window_train, y_train)\n",
    "train_results_TS42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TS42P, 'TS42P'),'train_results_TS42P_MLP')\n",
    "save(train_results_TS42P,'train_results_TS42P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TS42P[\"Actuals\"], train_results_TS42P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_MLP(model3, X_val, window_val, y_val)\n",
    "val_results_TS42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TS42P, 'TS42P'),'val_results_TS42P_MLP')\n",
    "save(val_results_TS42P,'val_results_TS42P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TS42P[\"Actuals\"], val_results_TS42P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_MLP(model3, X_test, window_test, y_test)\n",
    "test_results_TS42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS42P, 'TS42P'), 'test_results_TS42P_MLP')\n",
    "save(test_results_TS42P,'test_results_TS42P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TS42P[\"Actuals\"], test_results_TS42P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TS42P[\"Actuals\"], test_results_TS42P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model3 = LinearRegression()\n",
    "#Model training linear\n",
    "model3.fit(X_train, y_train)\n",
    "#model3 = load_model('model_TS42P/model_TS42P_LINEAR.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_LINEAR(model3, X_train, window_train, y_train)\n",
    "train_results_TS42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TS42P, 'TS42P'),'train_results_TS42P_LINEAR')\n",
    "save(train_results_TS42P,'train_results_TS42P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TS42P[\"Actuals\"], train_results_TS42P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_LINEAR(model3, X_val, window_val, y_val)\n",
    "val_results_TS42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TS42P, 'TS42P'),'val_results_TS42P_LINEAR')\n",
    "save(val_results_TS42P,'val_results_TS42P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TS42P[\"Actuals\"], val_results_TS42P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_LINEAR(model3, X_test, window_test, y_test)\n",
    "test_results_TS42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TS42P, 'TS42P'), 'test_results_TS42P_LINEAR')\n",
    "save(test_results_TS42P,'test_results_TS42P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TS42P[\"Actuals\"], test_results_TS42P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TS42P[\"Actuals\"], test_results_TS42P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_TS42P_MLP'), \"MODEL 3 - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P) MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TS42P_MLP'), get('val_results_TS42P_MLP'), get('test_results_TS42P_MLP'), \"MODEL 3 MLP ARX - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_TS42P_LINEAR'), \"MODEL 3 - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P) LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TS42P_LINEAR'), get('val_results_TS42P_LINEAR'), get('test_results_TS42P_LINEAR'), \"MODEL 3 LINEAR ARX - Model temperatury za schładzaczem 4 - TS42P (TS41P, GV4, FP3P))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 3 - Model temperatury za schładzaczem 4 - TS42P', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/TS42P_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/TS42P_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_TP42P_LSTM'), get('test_results_TP42P_MLP'), get('test_results_TP42P_LINEAR'), \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TP42P['TP42P'], WINDOW_SIZE)\n",
    "var = df_TP42P[['TS42P', 'B', 'FP4P']][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model(3) #3 Additional variables\n",
    "\n",
    "cp = ModelCheckpoint('model_TP42P/model_TP42P_raw_data.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model4.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model4 = load_model('model_TP42P/model_TP42P_raw_data.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "# train_predictions = predict(model4, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_TP42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(train_results_TP42P,'train_results_TP42P_normalized')\n",
    "# save(denorm(train_results_TP42P,'TP42P'),'train_results_TP42P')\n",
    "\n",
    "# val_predictions = predict(model4, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_TP42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(val_results_TP42P,'val_results_TP42P_normalized')\n",
    "# save(denorm(val_results_TP42P,'TP42P'),'val_results_TP42P')\n",
    "\n",
    "test_predictions = predict(model4, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TP42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(test_results_TP42P, 'test_results_TP42P_normalized_raw_data')\n",
    "#save(denorm(test_results_TP42P,'TP42P'), 'test_results_TP42P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display(get('train_results_TP42P'), get('val_results_TP42P'), get('test_results_TP42P_raw_data'),'test', \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "WINDOW_SIZE = 10\n",
    "X,y = data_to_X_y(df_TP42P_resampled['TP42P'], WINDOW_SIZE)\n",
    "var = df_TP42P_resampled[['TS42P', 'B', 'FP4P']][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "X_train, var_train, y_train, X_val, var_val, y_val, X_test, var_test, y_test = data_to_cross_val(X, var, y)\n",
    "\n",
    "#X_train.shape, var_train.shape, y_train.shape, X_val.shape, var_val.shape, y_val.shape, X_test.shape, var_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model(3) #3 Additional variables\n",
    "\n",
    "cp = ModelCheckpoint('model_TP42P/model_TP42P_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model4.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=50, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model4 = load_model('model_TP42P/model_TP42P_LSTM.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict(model4, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "train_results_TP42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TP42P, 'TP42P'),'train_results_TP42P_LSTM')\n",
    "save(train_results_TP42P,'train_results_TP42P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TP42P[\"Actuals\"], train_results_TP42P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict(model4, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_TP42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TP42P, 'TP42P'),'val_results_TP42P_LSTM')\n",
    "save(val_results_TP42P,'val_results_TP42P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TP42P[\"Actuals\"], val_results_TP42P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict(model4, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_TP42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP42P, 'TP42P'), 'test_results_TP42P_LSTM')\n",
    "save(test_results_TP42P,'test_results_TP42P_normalized_LSTM')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TP42P[\"Actuals\"], test_results_TP42P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TP42P[\"Actuals\"], test_results_TP42P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_TP42P_LSTM'), \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P) LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TP42P_LSTM'), get('val_results_TP42P_LSTM'), get('test_results_TP42P_LSTM'), \"MODEL 4 LSTM - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "size = df_TP42P_MLP.shape[0]\n",
    "train = int(size*0.8)\n",
    "val = int(size*0.9)\n",
    "\n",
    "X_train = df_TP42P_MLP[col_TP42P[1:] + ['y_k_1','y_k_2']][:train]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = df_TP42P_MLP[col_TP42P[0]][:train]\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "X_val = df_TP42P_MLP[col_TP42P[1:] + ['y_k_1','y_k_2']][train:val]\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = df_TP42P_MLP[col_TP42P[0]][train:val]\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "X_test = df_TP42P_MLP[col_TP42P[1:] + ['y_k_1','y_k_2']][val:]\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = df_TP42P_MLP[col_TP42P[0]][val:]\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model_MLP(5) #3+2y\n",
    "\n",
    "cp = ModelCheckpoint('model_TP42P/model_TP42P_MLP.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model4.fit(X_train, y_train, validation_data=(X_val,  y_val), epochs=10, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions MLP\n",
    "model4 = load_model('model_TP42P/model_TP42P_MLP.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_MLP(model4, X_train, window_train, y_train)\n",
    "train_results_TP42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TP42P, 'TP42P'),'train_results_TP42P_MLP')\n",
    "save(train_results_TP42P,'train_results_TP42P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TP42P[\"Actuals\"], train_results_TP42P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_MLP(model4, X_val, window_val, y_val)\n",
    "val_results_TP42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TP42P, 'TP42P'),'val_results_TP42P_MLP')\n",
    "save(val_results_TP42P,'val_results_TP42P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TP42P[\"Actuals\"], val_results_TP42P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_MLP(model4, X_test, window_test, y_test)\n",
    "test_results_TP42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP42P, 'TP42P'), 'test_results_TP42P_MLP')\n",
    "save(test_results_TP42P,'test_results_TP42P_normalized_MLP')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TP42P[\"Actuals\"], test_results_TP42P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TP42P[\"Actuals\"], test_results_TP42P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model4 = LinearRegression()\n",
    "#Model training linear\n",
    "model4.fit(X_train, y_train)\n",
    "#model4 = load_model('model_TP42P/model_TP42P_LINEAR.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "train_predictions = predict_LINEAR(model4, X_train, window_train, y_train)\n",
    "train_results_TP42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_TP42P, 'TP42P'),'train_results_TP42P_LINEAR')\n",
    "save(train_results_TP42P,'train_results_TP42P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(train_results_TP42P[\"Actuals\"], train_results_TP42P[\"Train Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "val_predictions = predict_LINEAR(model4, X_val, window_val, y_val)\n",
    "val_results_TP42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_TP42P, 'TP42P'),'val_results_TP42P_LINEAR')\n",
    "save(val_results_TP42P,'val_results_TP42P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(val_results_TP42P[\"Actuals\"], val_results_TP42P[\"Val Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "test_predictions = predict_LINEAR(model4, X_test, window_test, y_test)\n",
    "test_results_TP42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP42P, 'TP42P'), 'test_results_TP42P_LINEAR')\n",
    "save(test_results_TP42P,'test_results_TP42P_normalized_LINEAR')\n",
    "\n",
    "me, mse, mae, rmse = calculate_metrics(test_results_TP42P[\"Actuals\"], test_results_TP42P[\"Test Predictions\"])\n",
    "metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me, mse, mae, rmse = calculate_metrics(test_results_TP42P[\"Actuals\"], test_results_TP42P[\"Test Predictions\"])\n",
    "# metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_TP42P_MLP'), \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P) MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TP42P_MLP'), get('val_results_TP42P_MLP'), get('test_results_TP42P_MLP'), \"MODEL 4 MLP ARX - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_TP42P_LINEAR'), \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P) LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_TP42P_LINEAR'), get('val_results_TP42P_LINEAR'), get('test_results_TP42P_LINEAR'), \"MODEL 4 LINEAR ARX - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/TP42P_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/TP42P_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_equation_with_inputs(a1, a2, b1, b2, b3, x0, x1, input1, input2, input3, noise, n):\n",
    "    \"\"\"\n",
    "    Calculates values for a difference equation with three inputs:\n",
    "    x[i] = a1 * x[i-1] + a2 * x[i-2] + b1 * input1[i] + b2* input2[i] + b3* input3[i]) + noise\n",
    "    \n",
    "    Parameters:\n",
    "    a1, a2: autoregression coefficients\n",
    "    b1, b2, b3 : coefficients for input variables\n",
    "    c: constant term\n",
    "    x0, x1: initial conditions\n",
    "    input1, input2, input3: input arrays\n",
    "    n: number of iterations\n",
    "    \"\"\"\n",
    "    x = np.zeros(n)\n",
    "    x[0], x[1] = x0, x1\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        x[i] = a1 * x[i-1] + a2 * x[i-2] + b1 * input1[i] + b2* input2[i] + b3* input3[i] + noise[i]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example input data\n",
    "n = 5000\n",
    "input1 = np.sin(np.linspace(0, 5*np.pi, n))  # Sinusoidal signal\n",
    "input2 = -abs(np.cos(np.linspace(0, 10*np.pi, n ))/1.5)+0.25 #Cosinusoidal signal\n",
    "input3 = np.cumsum(np.random.randn(n))/10  # Scaled random walk\n",
    "noise = np.random.randn(n)/25  # Gaussian noise\n",
    "# Equation parameters\n",
    "a1, a2 = 0.5, -0.2  # autoregression coefficients\n",
    "b1, b2, b3 = -0.5, 0.7, 0.25 # coefficients for input variables\n",
    "#c = 0.2  # constant term\n",
    "x0, x1 = 0, 0.5  # initial conditions\n",
    "\n",
    "# Calculate values\n",
    "x = difference_equation_with_inputs(a1, a2, b1, b2, b3, x0, x1, input1, input2, input3, noise, n)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(5, 1, 1)\n",
    "plt.plot(range(n), x, 'b-')\n",
    "plt.title('Difference Equation Output')\n",
    "plt.ylabel('x[n]')\n",
    "\n",
    "plt.subplot(5, 1, 2)\n",
    "plt.plot(range(n), input1, 'g-')\n",
    "plt.title('Input 1 (Sinusoidal)')\n",
    "plt.ylabel('input1[n]')\n",
    "\n",
    "plt.subplot(5, 1, 3)\n",
    "plt.plot(range(n), input2, 'r-')\n",
    "plt.title('Input 2 (Cosinusoidal)')\n",
    "plt.ylabel('input2[n]')\n",
    "\n",
    "plt.subplot(5, 1, 4)\n",
    "plt.plot(range(n), input3, 'm-')\n",
    "plt.title('Input 3 (Scaled Random Walk)')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('input3[n]')\n",
    "\n",
    "plt.subplot(5, 1, 5)\n",
    "plt.plot(range(n), noise, 'y-')\n",
    "plt.title('Noise (Gaussian noise)')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('noise')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print first 10 values\n",
    "print(\"First 10 values:\")\n",
    "for i in range(10):\n",
    "    print(f\"x[{i}] = {x[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "  'x': x,\n",
    "  'input1': input1,\n",
    "  'input2': input2,\n",
    "  'input3': input3\n",
    "})\n",
    "col = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_k_1'] = df['x'].shift(1)  # y[k-1]\n",
    "df['y_k_2'] = df['x'].shift(2)  # y[k-2]\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {column:(min(df[column]),max(df[column])) for column in df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df\n",
    "df_raw.to_csv(\"check/df_raw_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_raw = noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column in df:\n",
    "  if(ranges[column] != (0, 0)): #jeśli jest podany zakres\n",
    "    df[[column]] = (df[[column]]-ranges[column][0])/(ranges[column][1]-ranges[column][0])\n",
    "noise = (noise_raw-noise_raw.min())/(noise_raw.max()-noise_raw.min())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"check/df_norm_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation (normalized data)\n",
    "size = df.shape[0]\n",
    "train = int(size*0.8)\n",
    "val = int(size*0.9)\n",
    "\n",
    "X_train = df[col[1:] + ['y_k_1','y_k_2']][:train]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = df[col[0]][:train]\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "X_val = df[col[1:] + ['y_k_1','y_k_2']][train:val]\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = df[col[0]][train:val]\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "\n",
    "X_test = df[col[1:] + ['y_k_1','y_k_2']][val:]\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = df[col[0]][val:]\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "noise_train = noise[2:train+2]\n",
    "noise_val = noise[train+2:val+2]\n",
    "noise_test = noise[val+2:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train, columns=['input1', 'input2', 'input3', 'y_k_1', 'y_k_2']).to_csv(\"check/X_train_norm.csv\")\n",
    "pd.DataFrame(X_val, columns=['input1', 'input2', 'input3', 'y_k_1', 'y_k_2']).to_csv(\"check/X_val_norm.csv\")\n",
    "pd.DataFrame(X_test, columns=['input1', 'input2', 'input3', 'y_k_1', 'y_k_2']).to_csv(\"check/X_test_norm.csv\")\n",
    "pd.DataFrame(y_train, columns=['y']).to_csv(\"check/y_train_norm.csv\")\n",
    "pd.DataFrame(y_val, columns=['y']).to_csv(\"check/y_val_norm.csv\")\n",
    "pd.DataFrame(y_test, columns=['y']).to_csv(\"check/y_test_norm.csv\")\n",
    "pd.DataFrame(noise_train, columns=['Gaussian noise']).to_csv(\"check/noise_train_norm.csv\")\n",
    "pd.DataFrame(noise_val, columns=['Gaussian noise']).to_csv(\"check/noise_val_norm.csv\")\n",
    "pd.DataFrame(noise_test, columns=['Gaussian noise']).to_csv(\"check/noise_test_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data check (raw data)\n",
    "size = df_raw.shape[0]\n",
    "train = int(size*0.8)\n",
    "val = int(size*0.9)\n",
    "\n",
    "X_train = df_raw[col[1:] + ['y_k_1','y_k_2']][:train]\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = df_raw[col[0]][:train]\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "X_val = df_raw[col[1:] + ['y_k_1','y_k_2']][train:val]\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = df_raw[col[0]][train:val]\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "X_test = df_raw[col[1:] + ['y_k_1','y_k_2']][val:]\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = df_raw[col[0]][val:]\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "noise_train = noise_raw[2:train+2]\n",
    "noise_val = noise_raw[train+2:val+2]\n",
    "noise_test = noise_raw[val+2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"check/df_raw_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a1, a2 = 0.5, -0.2  # autoregression coefficients\n",
    "b1, b2, b3 = -0.5, 0.7, 0.25 # coefficients for input variables\n",
    "\n",
    "i = 90\n",
    "input1 = 0\n",
    "input2 = 1\n",
    "input3 = 2\n",
    "y_k_1 = 3\n",
    "y_k_2 = 4\n",
    "\n",
    "x = a1 * X_test[i][y_k_1] + a2 * X_test[i][y_k_2] + b1 * X_test[i][input1] + b2* X_test[i][input2] + b3* X_test[i][input3] + noise_test[i]\n",
    "print(x)\n",
    "print(y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[i][input1])\n",
    "print(X_test[i][input2])\n",
    "print(X_test[i][input3])\n",
    "print(X_test[i][y_k_1])\n",
    "print(X_test[i][y_k_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2 = 0.5, -0.2  # autoregression coefficients\n",
    "b1, b2, b3 = -0.5, 0.7, 0.25 # coefficients for input variables\n",
    "\n",
    "\n",
    "x = a1 * 0.5 + a2 * 0 + b1 * 0.00628440082918321 + b2* -0.4166140084082909 + b3* 0.07316128932018406 + noise[2]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model_MLP_onelayer(5) #3+2y\n",
    "\n",
    "cp = ModelCheckpoint('check/MLP_onelayer_check.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "model4.fit(X_train, y_train, validation_data=(X_val,  y_val), epochs=30, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model4 = load_model('check/MLP_onelayer_check.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "# train_predictions = predict(model4, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_TP42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_TP42P, 'TP42P'),'train_results_TP42P_MLP')\n",
    "# save(train_results_TP42P,'train_results_TP42P_normalized_MLP')\n",
    "\n",
    "# val_predictions = predict(model4, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_TP42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_TP42P, 'TP42P'),'val_results_TP42P_MLP')\n",
    "# save(val_results_TP42P,'val_results_TP42P_normalized_MLP')\n",
    "\n",
    "test_predictions = predict_MLP(model4, X_test, window_test, y_test)\n",
    "test_results_TP42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP42P, 'x'), 'test_results_MLP_onelayer_check')\n",
    "save(test_results_TP42P,'test_results_MLP_onelayer_check_norm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model_linear = LinearRegression()\n",
    "#Model training linear\n",
    "model_linear.fit(X_train, y_train)\n",
    "#model_linear = load_model('model_TP42P/model_TP42P_LINEAR.keras')\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "\n",
    "# train_predictions = predict(model_linear, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_TP42P = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_TP42P, 'TP42P'),'train_results_TP42P_LINEAR')\n",
    "# save(train_results_TP42P,'train_results_TP42P_normalized_LINEAR')\n",
    "\n",
    "# val_predictions = predict(model_linear, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_TP42P = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_TP42P, 'TP42P'),'val_results_TP42P_LINEAR')\n",
    "# save(val_results_TP42P,'val_results_TP42P_normalized_LINEAR')\n",
    "\n",
    "test_predictions = predict_LINEAR(model_linear, X_test, window_test, y_test)\n",
    "test_results_TP42P = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_TP42P, 'x'), 'test_results_LinearRegression_check')\n",
    "save(test_results_TP42P,'test_results_LinearRegression_check_norm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.weights[0][0].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_linear.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame({\n",
    "  'model': ['diff_equation', 'MLP_ARX', 'LINEAR_ARX'],\n",
    "  'input1_weight': [b1, model4.weights[0][0].numpy()[0], list(model_linear.coef_)[0]],\n",
    "  'input2_weight': [b2, model4.weights[0][1].numpy()[0], list(model_linear.coef_)[1]],\n",
    "  'input3_weight': [b3, model4.weights[0][2].numpy()[0], list(model_linear.coef_)[2]],\n",
    "  'y_k_1_weight': [a1, model4.weights[0][3].numpy()[0], list(model_linear.coef_)[3]],\n",
    "  'y_k_2_weight': [a2, model4.weights[0][4].numpy()[0], list(model_linear.coef_)[4]]\n",
    "  \n",
    "})\n",
    "weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.to_csv('check/weights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame({\n",
    "  'model': ['diff_equation', 'MLP_ARX', 'LINEAR_ARX'],\n",
    "  'input1_weight': [0.3, 0.2550321, 0.08607239],\n",
    "  'input2_weight': [0.3, 0.13437146, 0.33218012],\n",
    "  'input3_weight': [0.3, 0.5927429,0.53543508],\n",
    "  'y_k_1_weight': [0.5, -0.23844196, 0.5],\n",
    "  'y_k_2_weight': [-0.2, 0.16206977, -0.2]\n",
    "  \n",
    "})\n",
    "weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display(get('train_results_TP42P'), get('val_results_TP42P'), get('test_results_MLP_onelayer_check'),'test', \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P) MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display(get('train_results_TP42P'), get('val_results_TP42P'), get('test_results_LinearRegression_check'),'test', \"MODEL 4 - Model temperatury za przegrzewaczem 4 - TP42P (TS42P, B, FP4P) LINEAR ARX\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
