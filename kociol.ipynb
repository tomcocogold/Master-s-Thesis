{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION AND METHODS LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z kotła\n",
    "# List of file paths\n",
    "rel_path = '..\\\\dane\\\\sodowy_no_missing_values\\\\'\n",
    "file_paths = ['part_326.csv', 'part_368.csv', 'part_369.csv',\n",
    "              'part_821.csv', 'part_822.csv', 'part_942.csv',\n",
    "              'part_2757.csv', 'part_2776.csv', 'part_2783.csv',\n",
    "              'part_2789.csv', 'part_2793.csv', 'part_2878.csv',\n",
    "              'part_2883.csv', 'part_3143.csv', 'part_3188.csv',\n",
    "              'part_3588.csv', 'part_3660.csv', 'part_4381.csv',\n",
    "              'part_4425.csv', 'part_6850.csv']\n",
    "file_paths = [rel_path + file for file in file_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File paths for each type of cross-val dataset\n",
    "train_file_paths = file_paths[:15]\n",
    "val_file_paths = file_paths[15:16]\n",
    "test_file_paths = file_paths[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za normalizację danych na podstawie podanych zakresów\n",
    "def norm(data):\n",
    "  data_norm = data.copy()\n",
    "  for column in data_norm:\n",
    "    if(ranges[column] != (0, 0)): #jeśli jest podany zakres\n",
    "      data_norm[[column]] = (data_norm[[column]]-ranges[column][0])/(ranges[column][1]-ranges[column][0])\n",
    "  return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytujemy dane z plików i sklejamy\n",
    "data = pd.concat([pd.read_csv(file, delimiter=';') for file in file_paths], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Słownik z zakresami dla danych z każdej kolumuny\n",
    "ranges = {column:(min(data[column]),max(data[column])) for column in data.columns}\n",
    "ranges['time'] = (0,0) #Czasu nie normalizujemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Połączone zbiory znormalizowanych danych treningowych, walidacyjnych i testowych bez wierszy wyznaczających granicę miedzy kolejnymi plikami\n",
    "train_file_data = pd.concat([norm(pd.read_csv(file, delimiter=';')).iloc[2:].reset_index(drop = True) for file in train_file_paths], ignore_index=True)\n",
    "val_file_data = pd.concat([norm(pd.read_csv(file, delimiter=';')).iloc[2:].reset_index(drop = True) for file in val_file_paths], ignore_index=True)\n",
    "test_file_data = pd.concat([norm(pd.read_csv(file, delimiter=';')).iloc[2:].reset_index(drop = True) for file in test_file_paths], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling (taking every 10th row)\n",
    "\n",
    "#Testing data\n",
    "test_file_data_resampled = test_file_data.iloc[::10]\n",
    "test_file_data_resampled.reset_index(inplace= True, drop= True)\n",
    "\n",
    "#Validation data\n",
    "val_file_data_resampled = val_file_data.iloc[::10]\n",
    "val_file_data_resampled.reset_index(inplace= True, drop= True)\n",
    "\n",
    "#Training data\n",
    "train_file_data_resampled = train_file_data.iloc[::10]\n",
    "train_file_data_resampled.reset_index(inplace= True, drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_data.shape, test_file_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_data_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['v_16001_M01_ACT_RPM_OUT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za denormalizację wyników\n",
    "def denorm(data_norm, pred_name):\n",
    "  data_denorm = data_norm.copy()\n",
    "  for column in data_denorm:\n",
    "    if(ranges[pred_name] != (0, 0)): #jeśli jest podany zakres\n",
    "      data_denorm[[column]] = (data_denorm[[column]]*(ranges[pred_name][1]-ranges[pred_name][0]))+ranges[pred_name][0]\n",
    "  return data_denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wyodrębnienie zbiorów danych niezbędnych do stworzenia każdego z modeli (jako listy nazw kolumn)\n",
    "\n",
    "#Model obrotów wentylatora WS1\n",
    "#v_16001_M01_ACT_RPM_OUT\n",
    "col_WS1 = ['v_16001_M01_ACT_RPM_OUT', 'v_11502_PIZ', 'v_11503_TI', 'v_16524_PI','v_16532_TI', 'v_16542_PIZ', 'v_16627_TI','v_17516_FIC','v_17546_FICZ','v_19587_FIZ']\n",
    "\n",
    "#Model obrotów wentylatora WS2\n",
    "#v_16002_M01_ACT_RPM_OUT\n",
    "col_WS2 = ['v_16002_M01_ACT_RPM_OUT', 'v_16542_PIZ', 'v_16563_PIZ', 'v_22684_FI', 'v_17516_FIC', 'v_16513_PI', 'v_16512_PI', 'v_17546_FICZ', 'v_11502_PIZ', 'v_16627_TI', 'v_16534_TI' ]\n",
    "\n",
    "#Model pomiaru ciśnienia KP1 w komorze paleniskowej\n",
    "#v_16505_PI\tCiśnienie na wyjsciu KP 1\n",
    "col_KP1 = ['v_16505_PI', 'v_16502_PIZ', 'v_16512_PI', 'v_16509_PIC_PIDA_SP', 'v_19603_FI']\n",
    "\n",
    "#Model pomiaru ciśnienia K01 na wyjściu z kotła\n",
    "#v_16512_PI\tCiśnienie w środku KO 1\n",
    "col_KO1 = ['v_16512_PI', 'v_16525_PI', 'v_22684_FI', 'v_17516_FIC','v_16627_TI', 'v_11502_PIZ', 'v_16510_TI', 'v_16511_TI', 'v_16533_TI', 'v_17546_FICZ', 'v_16532_TI', 'v_16629_TI', 'v_17501_FICZ', 'v_16528_AIZ', 'v_22559_TI', 'v_16529_AIZ', 'v_15554_FICZ', 'v_16505_PI', 'v_16502_PIZ']\n",
    "\n",
    "#Model pomiaru ciśnienia K02 na wyjściu z kotła\n",
    "#v_16513_PI\tCiśnienie w środku KO 2\n",
    "col_KO2 = ['v_16513_PI', 'v_16525_PI', 'v_22684_FI','v_16627_TI', 'v_11502_PIZ', 'v_16510_TI', 'v_16511_TI', 'v_16533_TI', 'v_17546_FICZ', 'v_16532_TI', 'v_17516_FIC', 'v_16629_TI', 'v_17501_FICZ', 'v_16528_AIZ', 'v_22559_TI', 'v_16529_AIZ', 'v_15554_FICZ', 'v_16502_PIZ']\n",
    "\n",
    "#Model pomiaru ciśnienia spalin KS1\n",
    "#v_16524_PI\tPodciśnienie na wejściu KS 1\n",
    "col_KS1 = ['v_16524_PI', 'v_16563_PIZ', 'v_16542_PIZ', 'v_17516_FIC', 'v_16512_PI']\n",
    "\n",
    "#Model pomiaru ciśnienia spalin KS2\n",
    "#v_16525_PI\tPodciśnienie na wejściu KS 2\n",
    "col_KS2 = ['v_16525_PI', 'v_16563_PIZ', 'v_16542_PIZ', 'v_17516_FIC', 'v_16512_PI', 'v_16513_PI']\n",
    "\n",
    "#Model pomiaru zawartości tlenu w spalinach O1\n",
    "#v_16528_AIZ\tTlen w spalinach w kotle pom 1\n",
    "col_O1 = ['v_16528_AIZ', 'v_19587_FIZ', 'v_22560_FIC', 'v_22684_FI', 'v_19588_TIZ', 'v_16511_TI', 'v_16510_TI', 'v_16509_PIC_PIDA_OP', 'v_16524_PI', 'v_16512_PI', 'v_16513_PI', 'v_19518_TIC', 'v_16627_TI', 'v_17516_FIC', 'v_15549_FICZ', 'v_19517_TIC', 'v_15557_FICZ']\n",
    "\n",
    "#Model pomiaru zawartości tlenu w spalinach O2\n",
    "#v_16529_AIZ\tTlen w spalinach w kotle pom 2\n",
    "col_O2 = ['v_16529_AIZ', 'v_19587_FIZ', 'v_22560_FIC', 'v_22684_FI', 'v_16510_TI', 'v_19588_TIZ', 'v_16509_PIC_PIDA_OP']\n",
    "\n",
    "#Model pomiaru zawartości tlenu w spalinach O3\n",
    "#v_16530_AIZ  Tlen w spalinach w kotle pom 3\n",
    "col_O3 = ['v_16530_AIZ', 'v_19587_FIZ', 'v_22560_FIC', 'v_22684_FI', 'v_16510_TI', 'v_19517_TIC', 'v_19588_TIZ', 'v_15549_FICZ', 'v_16509_PIC_PIDA_OP', 'v_16525_PI', 'v_16512_PI', 'v_16511_TI', 'v_15557_FICZ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja przetwarzająca szereg czasowy na próbki w postaci: window_size wartości poprzednich -> wartość następna\n",
    "def data_to_X_y(data, window_size=5):\n",
    "    data_as_np = data.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data_as_np)-window_size):\n",
    "        #Jako wejście bierzemy ciąg o długości WINDOW_SIZE-1\n",
    "        row = [[a] for a in data_as_np[i:i+window_size]]\n",
    "        X.append(row)\n",
    "        #Jako etykietę bierzemy następną wartość po ciągu\n",
    "        label = data_as_np[i+window_size]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za podział danych na zbiór treningowy, walidacyjny oraz testowy (przetwarzane są szeregi czasowe, więc nie mieszamy danych) \n",
    "##############\n",
    "#  W TYM PRZYPADKU MAMY JUŻ DANE PODZIELONE NA PLIKI, WIĘC TA FUNKCJA NIE BĘDZIE WYKORZYSTYWANA\n",
    "##############\n",
    "def data_to_cross_val(sequences, variables, labels):\n",
    "  #Pobieramy rozmiar zbioru danych wejściowych\n",
    "  size = sequences.shape[0]\n",
    "  train = int(size*0.8)\n",
    "  val = int(size*0.9)\n",
    "  \n",
    "  sequences_train, variables_train, labels_train = sequences[:train], variables[:train], labels[:train]\n",
    "  sequences_val, variables_val, labels_val = sequences[train:val], variables[train:val], labels[train:val]\n",
    "  sequences_test, variables_test, labels_test = sequences[val:], variables[val:], labels[val:]\n",
    "  \n",
    "  return sequences_train, variables_train, labels_train, sequences_val, variables_val, labels_val, sequences_test, variables_test, labels_test\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras imports\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za tworzenie modelu\n",
    "def create_model(var_num):\n",
    "    # Define input layers\n",
    "    sequence_input = Input(shape=(None, 1))  #Values for the past time steps (input can have any window size)\n",
    "    variables_input = Input(shape=(var_num,))  # Variables for the current time step\n",
    "\n",
    "    #LSTM layers with dropout for sequences\n",
    "    lstm_sequence = LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(sequence_input)\n",
    "    lstm_sequence = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(lstm_sequence)\n",
    "\n",
    "    #Dense layer for additional variables at current time step\n",
    "    dense_variables = Dense(32, activation='relu')(variables_input)\n",
    "    dense_variables = Dropout(0.2)(dense_variables)\n",
    "    \n",
    "    # Concatenate the outputs of both layers\n",
    "    merged = concatenate([lstm_sequence, dense_variables])\n",
    "    \n",
    "    # Dense layer for regression with linear activation function\n",
    "    output = Dense(1, activation='linear', kernel_regularizer='l2')(merged)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[sequence_input, variables_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za przewidywanie kolejnych wartości -> wersja ze średnią ważoną\n",
    "def predict(model, input_sequence, input_variables, window_size, num_steps, labels):\n",
    "    predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "    sequence_input = np.array(input_sequence[0]) #We take only the first window_size values from the data\n",
    "    variables_input = np.array(input_variables) #We create a temporary variable for additional variables data\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "        next_value = model.predict([sequence_input.reshape(1, window_size, 1), variables_input[i].reshape(1,variables_input.shape[1])], verbose=0) \n",
    "        \n",
    "        #Adding predicted values to predictions array\n",
    "        predictions[i] = next_value[0][0]\n",
    "        \n",
    "        # Update the input sequence for the next iteration\n",
    "        sequence_input = np.roll(sequence_input, -1)  # Shift left one place\n",
    "        sequence_input[-1] = ((9 * next_value[0][0] + labels[i]) / 10)  # Insert the new value (weighted average) at the end\n",
    "        \n",
    "        #Counting how many steps forward have already been predicted and printing it to the console\n",
    "        if i % 500 == 0:\n",
    "            #os.system('cls')\n",
    "            print(i, '/', num_steps)\n",
    "    print(\"Done\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Funkcja odpowiedzialna za przewidywanie kolejnych wartości -> wersja bez średniej ważonej\n",
    "# def predict(model, input_sequence, input_variables, window_size, num_steps):\n",
    "#     predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "#     sequence_input = np.array(input_sequence[0]) #We take only the first window_size values from the data\n",
    "#     variables_input = np.array(input_variables) #We create a temporary variable for additional variables data\n",
    "    \n",
    "#     for i in range(num_steps):\n",
    "        \n",
    "#         #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "#         next_value = model.predict([sequence_input.reshape(1, window_size, 1), variables_input[i].reshape(1,variables_input.shape[1])], verbose=0) \n",
    "        \n",
    "#         #Adding predicted values to predictions array\n",
    "#         predictions[i] = next_value[0][0]\n",
    "        \n",
    "#         # Update the input sequence for the next iteration\n",
    "#         sequence_input = np.roll(sequence_input, -1)  # Shift left one place\n",
    "#         sequence_input[-1] = next_value[0][0]  # Insert the new value at the end\n",
    "        \n",
    "#         #Counting how many steps forward have already been predicted and printing it to the console\n",
    "#         if i % 500 == 0:\n",
    "#             #os.system('cls')\n",
    "#             print(i, '/', num_steps)\n",
    "#     print(\"Done\")\n",
    "#     return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowedzialna za wyświetlanie wyników\n",
    "def display(train_results, val_results, test_results, which_dataset, plot_name):\n",
    "    if which_dataset=='all':\n",
    "        # Create a figure and subplots for each dataset from cross validation\n",
    "        fig, axs = plt.subplots(\n",
    "            3, 2,               # 3 rows, 2 columns\n",
    "            figsize=(20, 16),    # Width = 10 , height = 8 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0, 0].plot(train_results['Train Predictions'], label='Model')\n",
    "        axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "        axs[0, 0].set_title('Training dataset')\n",
    "        axs[0, 0].set_xlabel('Time')\n",
    "        axs[0, 0].set_ylabel('Value')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[1, 0].plot(val_results['Val Predictions'], label='Model')\n",
    "        axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "        axs[1, 0].set_title('Validation dataset')\n",
    "        axs[1, 0].set_xlabel('Time')\n",
    "        axs[1, 0].set_ylabel('Value')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "        axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "        axs[2, 0].set_title('Testing dataset')\n",
    "        axs[2, 0].set_xlabel('Time')\n",
    "        axs[2, 0].set_ylabel('Value')\n",
    "        axs[2, 0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[0, 1].plot(train_results['Train Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "        axs[0, 1].set_title('Training dataset (Difference)')\n",
    "        axs[0, 1].set_xlabel('Time')\n",
    "        axs[0, 1].set_ylabel('Value')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 1].plot(val_results['Val Predictions']- val_results['Actuals'], label='Difference', color='red' )\n",
    "        axs[1, 1].set_title('Validation dataset (Difference)')\n",
    "        axs[1, 1].set_xlabel('Time')\n",
    "        axs[1, 1].set_ylabel('Value')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "        axs[2, 1].set_title('Testing dataset (Difference)')\n",
    "        axs[2, 1].set_xlabel('Time')\n",
    "        axs[2, 1].set_ylabel('Value')\n",
    "        axs[2, 1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "        \n",
    "    elif which_dataset=='train':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(train_results['Train Predictions'], label='Model')\n",
    "        axs[0].plot(train_results['Actuals'], label='Real')\n",
    "        axs[0].set_title('Training dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(train_results['Train Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "        axs[1].set_title('Training dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Value')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    elif which_dataset=='val':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(val_results['Val Predictions'], label='Model')\n",
    "        axs[0].plot(val_results['Actuals'], label='Real')\n",
    "        axs[0].set_title('Validation dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(val_results['Val Predictions'] - val_results['Actuals'], label='Difference', color='red')\n",
    "        axs[1].set_title('Validation dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Value')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "    \n",
    "    elif which_dataset=='test':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(test_results['Test Predictions'], label='Model')\n",
    "        axs[0].plot(test_results['Actuals'], label='Real')\n",
    "        axs[0].set_title('Testing dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "        axs[1].set_title('Testing dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Value')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    else: print(\"Please enter a valid dataset to display!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za wyświetlanie wyników z podanego przedziału\n",
    "def display_window(train_results, val_results, test_results, which_dataset, plot_name, train=0, val=0, test=0, window_train=3000, window_val=3000, window_test=3000):\n",
    "    if which_dataset=='all':\n",
    "        # Create a figure and subplots for each dataset from cross validation\n",
    "        fig, axs = plt.subplots(\n",
    "            3, 2,               # 3 rows, 2 columns\n",
    "            figsize=(20, 16),    # Width = 10 , height = 8 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0, 0].plot(train_results['Train Predictions'][train:train+window_train], label='Model') #\n",
    "        axs[0, 0].plot(train_results['Actuals'][train:train+window_train], label='Real')\n",
    "        axs[0, 0].set_title('Training dataset')\n",
    "        axs[0, 0].set_xlabel('Time')\n",
    "        axs[0, 0].set_ylabel('Value')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[1, 0].plot(val_results['Val Predictions'][val:val+window_val], label='Model') #\n",
    "        axs[1, 0].plot(val_results['Actuals'][val:val+window_val], label='Real')\n",
    "        axs[1, 0].set_title('Validation dataset')\n",
    "        axs[1, 0].set_xlabel('Time')\n",
    "        axs[1, 0].set_ylabel('Value')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[2, 0].plot(test_results['Test Predictions'][test:test+window_test], label='Model') #\n",
    "        axs[2, 0].plot(test_results['Actuals'][test:test+window_test], label='Real')\n",
    "        axs[2, 0].set_title('Testing dataset')\n",
    "        axs[2, 0].set_xlabel('Time')\n",
    "        axs[2, 0].set_ylabel('Value')\n",
    "        axs[2, 0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[0, 1].plot(train_results['Train Predictions'][train:train+window_train] - train_results['Actuals'][train:train+window_train], label='Difference', color='red') #\n",
    "        axs[0, 1].set_title('Training dataset (Difference)')\n",
    "        axs[0, 1].set_xlabel('Time')\n",
    "        axs[0, 1].set_ylabel('Value')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 1].plot(val_results['Val Predictions'][val:val+window_val]- val_results['Actuals'][val:val+window_val], label='Difference', color='red' ) #\n",
    "        axs[1, 1].set_title('Validation dataset (Difference)')\n",
    "        axs[1, 1].set_xlabel('Time')\n",
    "        axs[1, 1].set_ylabel('Value')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        axs[2, 1].plot(test_results['Test Predictions'][test:test+window_test] - test_results['Actuals'][test:test+window_test], label='Difference', color='red') #\n",
    "        axs[2, 1].set_title('Testing dataset (Difference)')\n",
    "        axs[2, 1].set_xlabel('Time')\n",
    "        axs[2, 1].set_ylabel('Value')\n",
    "        axs[2, 1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "        \n",
    "    elif which_dataset=='train':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(train_results['Train Predictions'][train:train+window_train], label='Model')\n",
    "        axs[0].plot(train_results['Actuals'][train:train+window_train], label='Real')\n",
    "        axs[0].set_title('Training dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(train_results['Train Predictions'][train:train+window_train] - train_results['Actuals'][train:train+window_train], label='Difference', color='red')\n",
    "        axs[1].set_title('Training dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Value')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    elif which_dataset=='val':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(val_results['Val Predictions'][val:val+window_val], label='Model')\n",
    "        axs[0].plot(val_results['Actuals'][val:val+window_val], label='Real')\n",
    "        axs[0].set_title('Validation dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(val_results['Val Predictions'][val:val+window_val] - val_results['Actuals'][val:val+window_val], label='Difference', color='red')\n",
    "        axs[1].set_title('Validation dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Value')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "    \n",
    "    elif which_dataset=='test':\n",
    "        # Create a figure and subplots for testing dataset\n",
    "        fig, axs = plt.subplots(\n",
    "            2, 1,               # 2 rows, 1 column\n",
    "            figsize=(20, 5),    # Width = 20 , height = 5 \n",
    "            gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "        )\n",
    "\n",
    "        # Plot data on subplots\n",
    "        axs[0].plot(test_results['Test Predictions'][test:test+window_test], label='Model')\n",
    "        axs[0].plot(test_results['Actuals'][test:test+window_test], label='Real')\n",
    "        axs[0].set_title('Testing dataset')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel('Value')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot difference between model and real data\n",
    "        axs[1].plot(test_results['Test Predictions'][test:test+window_test] - test_results['Actuals'][test:test+window_test], label='Difference', color='red')\n",
    "        axs[1].set_title('Testing dataset (Difference)')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel('Value')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.suptitle(plot_name, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    else: print(\"Please enter a valid dataset and parameters to display!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za wyświetlanie wyników z testing dataset\n",
    "def display_test_results(test_results, plot_name):\n",
    "    # Create a figure and subplots for the test dataset\n",
    "    fig, axs = plt.subplots(\n",
    "        2, 1,               # 2 rows, 1 column\n",
    "        figsize=(20, 10),    # Width = 20, height = 10\n",
    "        gridspec_kw=dict(hspace=0.5)   # Vertical spacing between subplots\n",
    "    )\n",
    "\n",
    "    # Plot predictions vs actuals\n",
    "    axs[0].plot(test_results['Test Predictions'], label='Model Predictions')\n",
    "    axs[0].plot(test_results['Actuals'], label='Actual Values')\n",
    "    axs[0].set_title('Testing Dataset Predictions vs Actuals')\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_ylabel('Value')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot the difference between predictions and actuals\n",
    "    axs[1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Prediction Error', color='red')\n",
    "    axs[1].set_title('Testing Dataset Prediction Error')\n",
    "    axs[1].set_xlabel('Time')\n",
    "    axs[1].set_ylabel('Value')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Set the main title and show the plot\n",
    "    fig.suptitle(plot_name, fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za zapisywanie dataframe do csv\n",
    "def save(df, file_name):\n",
    "  df.to_csv('predictions_new/'+file_name+'.csv')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za odczytywanie dataframe z csv\n",
    "def get(file_name):\n",
    "  return pd.read_csv('predictions_new/'+file_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Porównanie wykresów\n",
    "# #Funkcja odpowedzialna za wyświetlanie wyników\n",
    "# def display_comparison(train_results, val_results, test_results, plot_name):\n",
    "#         # Create a figure and subplots for each dataset from cross validation\n",
    "#         fig, axs = plt.subplots(\n",
    "#             3, 2,               # 3 rows, 2 columns\n",
    "#             figsize=(20, 16),    # Width = 10 , height = 8 \n",
    "#             gridspec_kw=dict(hspace=0.5, wspace=0.5)   # Spacing between subplots\n",
    "#         )\n",
    "\n",
    "#         # Plot data on subplots\n",
    "#         axs[0, 0].plot(train_results['Test Predictions'], label='Model')\n",
    "#         axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "#         axs[0, 0].set_title('Testing dataset LSTM')\n",
    "#         axs[0, 0].set_xlabel('Time')\n",
    "#         axs[0, 0].set_ylabel('Value')\n",
    "#         axs[0, 0].legend()\n",
    "\n",
    "#         axs[1, 0].plot(val_results['Test Predictions'], label='Model')\n",
    "#         axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "#         axs[1, 0].set_title('Testing dataset MLP ARX')\n",
    "#         axs[1, 0].set_xlabel('Time')\n",
    "#         axs[1, 0].set_ylabel('Value')\n",
    "#         axs[1, 0].legend()\n",
    "\n",
    "#         axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "#         axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "#         axs[2, 0].set_title('Testing dataset LINEAR ARX')\n",
    "#         axs[2, 0].set_xlabel('Time')\n",
    "#         axs[2, 0].set_ylabel('Value')\n",
    "#         axs[2, 0].legend()\n",
    "\n",
    "#         # Plot difference between model and real data\n",
    "#         axs[0, 1].plot(train_results['Test Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "#         axs[0, 1].set_title('Testing dataset LSTM (Difference)')\n",
    "#         axs[0, 1].set_xlabel('Time')\n",
    "#         axs[0, 1].set_ylabel('Value')\n",
    "#         axs[0, 1].legend()\n",
    "\n",
    "#         axs[1, 1].plot(val_results['Test Predictions']- val_results['Actuals'], label='Difference', color='red' )\n",
    "#         axs[1, 1].set_title('Testing dataset MLP ARX (Difference)')\n",
    "#         axs[1, 1].set_xlabel('Time')\n",
    "#         axs[1, 1].set_ylabel('Value')\n",
    "#         axs[1, 1].legend()\n",
    "\n",
    "#         axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "#         axs[2, 1].set_title('Testing dataset LINEAR ARX (Difference)')\n",
    "#         axs[2, 1].set_xlabel('Time')\n",
    "#         axs[2, 1].set_ylabel('Value')\n",
    "#         axs[2, 1].legend()\n",
    "\n",
    "#         fig.suptitle(plot_name, fontsize=16)\n",
    "#         plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porównanie wykresów + jednostki\n",
    "\n",
    "def display_comparison(train_results, val_results, test_results, plot_name, y_unit='°C', x_unit='100s'):\n",
    "    # Create a figure and subplots for each dataset from cross validation\n",
    "    fig, axs = plt.subplots(\n",
    "        3, 2,               \n",
    "        figsize=(20, 16),    \n",
    "        gridspec_kw=dict(hspace=0.5, wspace=0.5)   \n",
    "    )\n",
    "\n",
    "    # Plot data on subplots\n",
    "    axs[0, 0].plot(train_results['Test Predictions'], label='Model')\n",
    "    axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "    axs[0, 0].set_title('Testing dataset LSTM')\n",
    "    axs[0, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 0].set_ylabel(f'Value [{y_unit}]')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[1, 0].plot(val_results['Test Predictions'], label='Model')\n",
    "    axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "    axs[1, 0].set_title('Testing dataset MLP ARX')\n",
    "    axs[1, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 0].set_ylabel(f'Value [{y_unit}]')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "    axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "    axs[2, 0].set_title('Testing dataset LINEAR ARX')\n",
    "    axs[2, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 0].set_ylabel(f'Value [{y_unit}]')\n",
    "    axs[2, 0].legend()\n",
    "\n",
    "    # Plot difference between model and real data\n",
    "    axs[0, 1].plot(train_results['Test Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "    axs[0, 1].set_title('Testing dataset LSTM (Difference)')\n",
    "    axs[0, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 1].plot(val_results['Test Predictions'] - val_results['Actuals'], label='Difference', color='red')\n",
    "    axs[1, 1].set_title('Testing dataset MLP ARX (Difference)')\n",
    "    axs[1, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "    axs[2, 1].set_title('Testing dataset LINEAR ARX (Difference)')\n",
    "    axs[2, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[2, 1].legend()\n",
    "\n",
    "    fig.suptitle(plot_name, fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porównanie wykresów wyników jednego modelu (train, val, test) + jednostki\n",
    "\n",
    "def display_train_val_test(train_results, val_results, test_results, plot_name, y_unit='°C', x_unit='100s'):\n",
    "    # Create a figure and subplots for each dataset from cross validation\n",
    "    fig, axs = plt.subplots(\n",
    "        3, 2,               \n",
    "        figsize=(20, 16),    \n",
    "        gridspec_kw=dict(hspace=0.5, wspace=0.5)\n",
    "    )\n",
    "\n",
    "    # Plot data on subplots\n",
    "    axs[0, 0].plot(train_results['Train Predictions'], label='Model')\n",
    "    axs[0, 0].plot(train_results['Actuals'], label='Real')\n",
    "    axs[0, 0].set_title('Training dataset')\n",
    "    axs[0, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 0].set_ylabel(f'Value [{y_unit}]')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[1, 0].plot(val_results['Val Predictions'], label='Model')\n",
    "    axs[1, 0].plot(val_results['Actuals'], label='Real')\n",
    "    axs[1, 0].set_title('Validation dataset')\n",
    "    axs[1, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 0].set_ylabel(f'Value [{y_unit}]')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[2, 0].plot(test_results['Test Predictions'], label='Model')\n",
    "    axs[2, 0].plot(test_results['Actuals'], label='Real')\n",
    "    axs[2, 0].set_title('Testing dataset')\n",
    "    axs[2, 0].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 0].set_ylabel(f'Value [{y_unit}]')\n",
    "    axs[2, 0].legend()\n",
    "\n",
    "    # Plot difference between model and real data\n",
    "    axs[0, 1].plot(train_results['Train Predictions'] - train_results['Actuals'], label='Difference', color='red')\n",
    "    axs[0, 1].set_title('Training dataset (Difference)')\n",
    "    axs[0, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[0, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 1].plot(val_results['Val Predictions'] - val_results['Actuals'], label='Difference', color='red')\n",
    "    axs[1, 1].set_title('Validation dataset (Difference)')\n",
    "    axs[1, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[1, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    axs[2, 1].plot(test_results['Test Predictions'] - test_results['Actuals'], label='Difference', color='red')\n",
    "    axs[2, 1].set_title('Testing dataset (Difference)')\n",
    "    axs[2, 1].set_xlabel(f'Time [{x_unit}]')\n",
    "    axs[2, 1].set_ylabel(f'Difference [{y_unit}]')\n",
    "    axs[2, 1].legend()\n",
    "\n",
    "    fig.suptitle(plot_name, fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    me = (y_true - y_pred).mean()\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    mae = (y_true - y_pred).abs().mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    return me, mse, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min_green(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION AND METHODS MLP ARX AND LINEAR ARX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling and adding shifted output values as inputs\n",
    "#Testing data\n",
    "test_file_data_MLP = test_file_data.iloc[::10]\n",
    "test_file_data_MLP.reset_index(inplace= True, drop= True)\n",
    "\n",
    "test_file_data_MLP['y_k_1'] = test_file_data_MLP[col_WS1[0]].shift(1)  # y[k-1]\n",
    "test_file_data_MLP['y_k_2'] = test_file_data_MLP[col_WS1[0]].shift(2)  # y[k-2]\n",
    "test_file_data_MLP = test_file_data_MLP.dropna().reset_index(drop=True)\n",
    "\n",
    "#Validation data\n",
    "val_file_data_MLP = val_file_data.iloc[::10]\n",
    "val_file_data_MLP.reset_index(inplace= True, drop= True)\n",
    "\n",
    "val_file_data_MLP['y_k_1'] = val_file_data_MLP[col_WS1[0]].shift(1)  # y[k-1]\n",
    "val_file_data_MLP['y_k_2'] = val_file_data_MLP[col_WS1[0]].shift(2)  # y[k-2]\n",
    "val_file_data_MLP = val_file_data_MLP.dropna().reset_index(drop=True)\n",
    "\n",
    "#Training data\n",
    "train_file_data_MLP = train_file_data.iloc[::10]\n",
    "train_file_data_MLP.reset_index(inplace= True, drop= True)\n",
    "\n",
    "train_file_data_MLP['y_k_1'] = train_file_data_MLP[col_WS1[0]].shift(1)  # y[k-1]\n",
    "train_file_data_MLP['y_k_2'] = train_file_data_MLP[col_WS1[0]].shift(2)  # y[k-2]\n",
    "train_file_data_MLP = train_file_data_MLP.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za tworzenie modelu MLP\n",
    "def create_model_MLP(var_num):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation = 'relu', input_dim = var_num))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['rmse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za przewidywanie kolejnych wartości MLP ARX-> wersja ze średnią ważoną\n",
    "def predict_MLP(model, input, num_steps, labels):\n",
    "    predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "    input = np.array(input) #We create a temporary variable for input\n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "        next_value = model.predict(input[i].reshape(1,input.shape[1]), verbose=0) \n",
    "        \n",
    "        #Adding predicted values to predictions array\n",
    "        predictions[i] = next_value[0][0]\n",
    "        \n",
    "        #Switching y[k-1] and y[k-2] values for those predicted by the model\n",
    "        if i<num_steps-1:\n",
    "           input[i+1][input.shape[1]-2] = ((9 * predictions[i] + labels[i]) / 10) #y[k-1] one before last column\n",
    "           if i!=0:\n",
    "               input[i+1][input.shape[1]-1] = ((9 * predictions[i-1] + labels[i-1]) / 10)#y[k-2] last column\n",
    "           else:\n",
    "               pass\n",
    "        \n",
    "        \n",
    "        #Counting how many steps forward have already been predicted and printing it to the console\n",
    "        if i % 500 == 0:\n",
    "            #os.system('cls')\n",
    "            print(i, '/', num_steps)\n",
    "    print(\"Done\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Funkcja odpowiedzialna za przewidywanie kolejnych wartości MLP ARX -> wersja bez średniej ważonej\n",
    "# def predict_MLP(model, input, num_steps):\n",
    "#     predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "#     input = np.array(input) #We create a temporary variable for input\n",
    "    \n",
    "#     for i in range(num_steps):\n",
    "        \n",
    "#         #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "#         next_value = model.predict(input[i].reshape(1,input.shape[1]), verbose=0) \n",
    "        \n",
    "#         #Adding predicted values to predictions array\n",
    "#         predictions[i] = next_value[0][0]\n",
    "        \n",
    "#         #Switching y[k-1] and y[k-2] values for those predicted by the model\n",
    "#         if i<num_steps-1:\n",
    "#            input[i+1][input.shape[1]-2] = predictions[i]#y[k-1] one before last column\n",
    "#            if i!=0:\n",
    "#                input[i+1][input.shape[1]-1] = predictions[i-1]#y[k-2] last column\n",
    "#            else:\n",
    "#                pass\n",
    "        \n",
    "#         #Counting how many steps forward have already been predicted and printing it to the console\n",
    "#         if i % 500 == 0:\n",
    "#             #os.system('cls')\n",
    "#             print(i, '/', num_steps)\n",
    "#     print(\"Done\")\n",
    "#     return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja odpowiedzialna za przewidywanie kolejnych wartości LINEAR ARX-> wersja ze średnią ważoną\n",
    "def predict_LINEAR(model, input, num_steps, labels):\n",
    "    predictions = np.zeros(num_steps) #empty numpy array for storing predictions\n",
    "    input = np.array(input) #We create a temporary variable for input\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        #The data needs to be reshaped before passing into the model so that it fits earlier defined input sizes\n",
    "        next_value = model.predict(input[i].reshape(1,input.shape[1])) \n",
    "        \n",
    "        #Adding predicted values to predictions array\n",
    "        predictions[i] = next_value[0]\n",
    "        \n",
    "        #Switching y[k-1] and y[k-2] values for those predicted by the model\n",
    "    \n",
    "        if i<num_steps-1: \n",
    "           input[i+1][input.shape[1]-2] = ((9 * predictions[i] + labels[i]) / 10) #y[k-1] one before last column\n",
    "           if i!=0:\n",
    "               input[i+1][input.shape[1]-1] = ((9 * predictions[i-1] + labels[i-1]) / 10)#y[k-2] last column\n",
    "           else:\n",
    "               pass\n",
    "        \n",
    "        #Counting how many steps forward have already been predicted and printing it to the console\n",
    "        if i % 500 == 0:\n",
    "            #os.system('cls')\n",
    "            print(i, '/', num_steps)\n",
    "    print(\"Done\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 - Model obrotów wentylatora WS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_WS1_LSTM'), get('test_results_WS1_MLP'), get('test_results_WS1_LINEAR'), \"MODEL 1 - Model obrotów wentylatora WS1\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_WS1 = file_data[col_WS1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_WS1[col_WS1[0]], WINDOW_SIZE)\n",
    "  var_train = df_WS1[col_WS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train] \n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_WS1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_WS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model1 = create_model(9) #9 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_WS1/model_WS1.keras', save_best_only=True)\n",
    "\n",
    "model1.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model1.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model1 = load_model('model_WS1/model_WS1.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_WS1[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_WS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model1, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_WS1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_WS1, col_WS1[0]),'train_results_WS1')\n",
    "# save(train_results_WS1,'train_results_WS1_normalized')\n",
    "\n",
    "# val_predictions = predict(model1, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_WS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_WS1, col_WS1[0]),'val_results_WS1')\n",
    "# save(val_results_WS1,'val_results_WS1_normalized')\n",
    "\n",
    "test_predictions = predict(model1, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "test_results_WS1 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_WS1, col_WS1[0]), 'test_results_WS1')\n",
    "save(test_results_WS1,'test_results_WS1_normalized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_WS1'), \"MODEL 1 - Model obrotów wentylatora WS1 LSTM\")\n",
    "#display(get('train_results_WS1'), get('val_results_WS1'), get('test_results_WS1'),'all', \"MODEL 1 - Model obrotów wentylatora WS1 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_WS1 = file_data[col_WS1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_WS1[col_WS1[0]], WINDOW_SIZE)\n",
    "  var_train = df_WS1[col_WS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train] \n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_WS1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_WS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_WS1 = file_data[col_WS1]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_WS1[col_WS1[0]], WINDOW_SIZE)\n",
    "  var_test = df_WS1[col_WS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model1 = create_model(9) #9 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_WS1/model_WS1_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model1.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model1.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=20, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model1 = load_model('model_WS1/model_WS1_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model1, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_WS1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_WS1, col_WS1[0]), 'train_results_WS1_LSTM')\n",
    "save(train_results_WS1,'train_results_WS1_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model1, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_WS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_WS1, col_WS1[0]),'val_results_WS1_LSTM')\n",
    "save(val_results_WS1,'val_results_WS1_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model1, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_WS1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_WS1, col_WS1[0]), 'test_results_WS1_LSTM')\n",
    "save(test_results_WS1,'test_results_WS1_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_WS1_LSTM_test'), \"MODEL 1 - Model obrotów wentylatora WS1 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_WS1_LSTM'), get('val_results_WS1_LSTM'), get('test_results_WS1_LSTM'), \"MODEL 1 LSTM - Model obrotów wentylatora WS1\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_WS1 = file_data[col_WS1]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_WS1['y_k_1'] = df_WS1[col_WS1[0]].shift(1)  # y[k-1]\n",
    "  df_WS1['y_k_2'] = df_WS1[col_WS1[0]].shift(2)  # y[k-2]\n",
    "  df_WS1 = df_WS1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_WS1[col_WS1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_WS1[col_WS1[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_WS1[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_WS1[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_WS1 = file_data[col_WS1]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_WS1['y_k_1'] = df_WS1[col_WS1[0]].shift(1)  # y[k-1]\n",
    "  df_WS1['y_k_2'] = df_WS1[col_WS1[0]].shift(2)  # y[k-2]\n",
    "  df_WS1 = df_WS1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_WS1[col_WS1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_WS1[col_WS1[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation MLP\n",
    "model1 = create_model_MLP(11) #11 additional variables (9 variables + 2 earlier model outputs y[k-1] and y[k-2])\n",
    "\n",
    "cp = ModelCheckpoint('model_WS1/model_WS1_MLP.keras', save_best_only=True)\n",
    "\n",
    "model1.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model1 = load_model('model_WS1/model_WS1_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model1, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_WS1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_WS1, col_WS1[0]), 'train_results_WS1_MLP')\n",
    "save(train_results_WS1,'train_results_WS1_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model1, X_val, window_val, y_val)\n",
    "val_results_WS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_WS1, col_WS1[0]),'val_results_WS1_MLP')\n",
    "save(val_results_WS1,'val_results_WS1_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model1, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_WS1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_WS1, col_WS1[0]), 'test_results_WS1_MLP')\n",
    "save(test_results_WS1,'test_results_WS1_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model1 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_WS1[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_WS1[0]]\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model1, X_train, window_train, y_train)\n",
    "train_results_WS1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_WS1, col_WS1[0]),'train_results_WS1_LINEAR')\n",
    "save(train_results_WS1,'train_results_WS1_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model1, X_val, window_val, y_val)\n",
    "val_results_WS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_WS1, col_WS1[0]),'val_results_WS1_LINEAR')\n",
    "save(val_results_WS1,'val_results_WS1_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model1, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_WS1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_WS1, col_WS1[0]), 'test_results_WS1_LINEAR')\n",
    "save(test_results_WS1,'test_results_WS1_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_WS1_MLP'), \"MODEL 1 - Model obrotów wentylatora WS1 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_WS1_MLP'), get('val_results_WS1_MLP'), get('test_results_WS1_MLP'), \"MODEL 1 MLP ARX - Model obrotów wentylatora WS1\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_WS1_LINEAR'), \"MODEL 1 - Model obrotów wentylatora WS1 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_WS1_LINEAR'), get('val_results_WS1_LINEAR'), get('test_results_WS1_LINEAR'), \"MODEL 1 LINEAR ARX - Model obrotów wentylatora WS1\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_WS1_normalized_LSTM\", \"val_results_WS1_normalized_LSTM\", \"test_results_WS1_normalized_LSTM\", \"train_results_WS1_normalized_MLP\", \"val_results_WS1_normalized_MLP\", \"test_results_WS1_normalized_MLP\", \"train_results_WS1_normalized_LINEAR\", \"val_results_WS1_normalized_LINEAR\", \"test_results_WS1_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 1 - Model obrotów wentylatora WS1', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/WS1_models_metrics_NEW.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/WS1_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2 - Model obrotów wentylatora WS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_WS2_LSTM'), get('test_results_WS2_MLP'), get('test_results_WS2_LINEAR'), \"MODEL 2 - Model obrotów wentylatora WS2\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_WS2 = file_data[col_WS2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_WS2[col_WS2[0]], WINDOW_SIZE)\n",
    "  var_train = df_WS2[col_WS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_WS2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_WS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model2 = create_model(10) #10 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_WS2/model_WS2.keras', save_best_only=True)\n",
    "\n",
    "model2.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model2.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model2 = load_model('model_WS2/model_WS2.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_WS2[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_WS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model2, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_WS2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_WS2, col_WS2[0]),'train_results_WS2')\n",
    "# save(train_results_WS2,'train_results_WS2_normalized')\n",
    "\n",
    "# val_predictions = predict(model2, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_WS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_WS2, col_WS2[0]),'val_results_WS2')\n",
    "# save(val_results_WS2,'val_results_WS2_normalized')\n",
    "\n",
    "test_predictions = predict(model2, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_WS2 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_WS2, col_WS2[0]), 'test_results_WS2')\n",
    "save(test_results_WS2,'test_results_WS2_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_WS2'), \"MODEL 2 - Model obrotów wentylatora WS2 LSTM\")\n",
    "#display(get('train_results_WS2'), get('val_results_WS2'), get('test_results_WS2'),'all', \"MODEL 2 - Model obrotów wentylatora WS2 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_WS2 = file_data[col_WS2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_WS2[col_WS2[0]], WINDOW_SIZE)\n",
    "  var_train = df_WS2[col_WS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_WS2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_WS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_WS2 = file_data[col_WS2]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_WS2[col_WS2[0]], WINDOW_SIZE)\n",
    "  var_test = df_WS2[col_WS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model2 = create_model(10) #10 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_WS2/model_WS2_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model2.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model2.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model2 = load_model('model_WS2/model_WS2_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model2, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_WS2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_WS2, col_WS2[0]), 'train_results_WS2_LSTM')\n",
    "save(train_results_WS2,'train_results_WS2_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model2, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_WS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_WS2, col_WS2[0]),'val_results_WS2_LSTM')\n",
    "save(val_results_WS2,'val_results_WS2_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model2, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_WS2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_WS2, col_WS2[0]), 'test_results_WS2_LSTM')\n",
    "save(test_results_WS2,'test_results_WS2_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_WS2_LSTM'), \"MODEL 2 - Model obrotów wentylatora WS2 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_WS2_LSTM'), get('val_results_WS2_LSTM'), get('test_results_WS2_LSTM'), \"MODEL 2 LSTM - Model obrotów wentylatora WS2\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_WS2 = file_data[col_WS2]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_WS2['y_k_1'] = df_WS2[col_WS2[0]].shift(1)  # y[k-1]\n",
    "  df_WS2['y_k_2'] = df_WS2[col_WS2[0]].shift(2)  # y[k-2]\n",
    "  df_WS2 = df_WS2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_WS2[col_WS2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_WS2[col_WS2[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_WS2[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_WS2[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_WS2 = file_data[col_WS2]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_WS2['y_k_1'] = df_WS2[col_WS2[0]].shift(1)  # y[k-1]\n",
    "  df_WS2['y_k_2'] = df_WS2[col_WS2[0]].shift(2)  # y[k-2]\n",
    "  df_WS2 = df_WS2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_WS2[col_WS2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_WS2[col_WS2[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model2 = create_model_MLP(12) #12  variables (10 variables + 2 earlier model outputs y[k-1] and y[k-2])\n",
    "\n",
    "cp = ModelCheckpoint('model_WS2/model_WS2_MLP.keras', save_best_only=True)\n",
    "\n",
    "model2.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model2 = load_model('model_WS2/model_WS2_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model2, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_WS2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_WS2, col_WS2[0]), 'train_results_WS2_MLP')\n",
    "save(train_results_WS2,'train_results_WS2_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model2, X_val, window_val, y_val)\n",
    "val_results_WS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_WS2, col_WS2[0]),'val_results_WS2_MLP')\n",
    "save(val_results_WS2,'val_results_WS2_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model2, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_WS2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_WS2, col_WS2[0]), 'test_results_WS2_MLP')\n",
    "save(test_results_WS2,'test_results_WS2_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model2 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_WS2[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_WS2[0]]\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model2, X_train, window_train, y_train)\n",
    "train_results_WS2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_WS2, col_WS2[0]),'train_results_WS2_LINEAR')\n",
    "save(train_results_WS2,'train_results_WS2_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model2, X_val, window_val, y_val)\n",
    "val_results_WS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_WS2, col_WS2[0]),'val_results_WS2_LINEAR')\n",
    "save(val_results_WS2,'val_results_WS2_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model2, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_WS2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_WS2, col_WS2[0]), 'test_results_WS2_LINEAR')\n",
    "save(test_results_WS2,'test_results_WS2_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_WS2_MLP'), \"MODEL 2 - Model obrotów wentylatora WS2 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_WS2_MLP'), get('val_results_WS2_MLP'), get('test_results_WS2_MLP'), \"MODEL 2 MLP ARX - Model obrotów wentylatora WS2\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_WS2_LINEAR'), \"MODEL 2 - Model obrotów wentylatora WS2 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_WS2_LINEAR'), get('val_results_WS2_LINEAR'), get('test_results_WS2_LINEAR'), \"MODEL 2 LINEAR ARX - Model obrotów wentylatora WS2\", y_unit='rpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_WS2_normalized_LSTM\", \"val_results_WS2_normalized_LSTM\", \"test_results_WS2_normalized_LSTM\", \"train_results_WS2_normalized_MLP\", \"val_results_WS2_normalized_MLP\", \"test_results_WS2_normalized_MLP\", \"train_results_WS2_normalized_LINEAR\", \"val_results_WS2_normalized_LINEAR\", \"test_results_WS2_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 2 - Model obrotów wentylatora WS2', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/WS2_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/WS2_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_KP1_LSTM'), get('test_results_KP1_MLP'), get('test_results_KP1_LINEAR'), \"MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_KP1 = file_data[col_KP1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KP1[col_KP1[0]], WINDOW_SIZE)\n",
    "  var_train = df_KP1[col_KP1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_KP1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_KP1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model3 = create_model(4) #4 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KP1/model_KP1.keras', save_best_only=True)\n",
    "\n",
    "model3.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model3.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model3 = load_model('model_KP1/model_KP1.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_KP1[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_KP1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model3, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_KP1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_KP1, col_KP1[0]),'train_results_KP1')\n",
    "# save(train_results_KP1,'train_results_KP1_normalized')\n",
    "\n",
    "# val_predictions = predict(model3, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_KP1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_KP1, col_KP1[0]),'val_results_KP1')\n",
    "# save(val_results_KP1,'val_results_KP1_normalized')\n",
    "\n",
    "test_predictions = predict(model3, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_KP1 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_KP1, col_KP1[0]), 'test_results_KP1')\n",
    "save(test_results_KP1,'test_results_KP1_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KP1'), \"MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej LSTM\")\n",
    "#display(get('train_results_KP1'), get('val_results_KP1'), get('test_results_KP1'),'all', \"MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_KP1 = file_data[col_KP1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KP1[col_KP1[0]], WINDOW_SIZE)\n",
    "  var_train = df_KP1[col_KP1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_KP1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_KP1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KP1 = file_data[col_KP1]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_KP1[col_KP1[0]], WINDOW_SIZE)\n",
    "  var_test = df_KP1[col_KP1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model3 = create_model(4) #4 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KP1/model_KP1_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model3.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model3.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model3 = load_model('model_KP1/model_KP1_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model3, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_KP1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KP1, col_KP1[0]), 'train_results_KP1_LSTM')\n",
    "save(train_results_KP1,'train_results_KP1_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model3, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_KP1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KP1, col_KP1[0]),'val_results_KP1_LSTM')\n",
    "save(val_results_KP1,'val_results_KP1_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model3, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_KP1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KP1, col_KP1[0]), 'test_results_KP1_LSTM')\n",
    "save(test_results_KP1,'test_results_KP1_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KP1_LSTM'), \"MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KP1_LSTM'), get('val_results_KP1_LSTM'), get('test_results_KP1_LSTM'), \"MODEL 3 LSTM - Model pomiaru ciśnienia KP1 w komorze paleniskowej\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_KP1 = file_data[col_KP1]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KP1['y_k_1'] = df_KP1[col_KP1[0]].shift(1)  # y[k-1]\n",
    "  df_KP1['y_k_2'] = df_KP1[col_KP1[0]].shift(2)  # y[k-2]\n",
    "  df_KP1 = df_KP1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_KP1[col_KP1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_KP1[col_KP1[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_KP1[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_KP1[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KP1 = file_data[col_KP1]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KP1['y_k_1'] = df_KP1[col_KP1[0]].shift(1)  # y[k-1]\n",
    "  df_KP1['y_k_2'] = df_KP1[col_KP1[0]].shift(2)  # y[k-2]\n",
    "  df_KP1 = df_KP1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_KP1[col_KP1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_KP1[col_KP1[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model3 = create_model_MLP(6) #6 variables (4+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_KP1/model_KP1_MLP.keras', save_best_only=True)\n",
    "\n",
    "model3.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model3.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model3 = load_model('model_KP1/model_KP1_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model3, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_KP1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KP1, col_KP1[0]), 'train_results_KP1_MLP')\n",
    "save(train_results_KP1,'train_results_KP1_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model3, X_val, window_val, y_val)\n",
    "val_results_KP1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KP1, col_KP1[0]),'val_results_KP1_MLP')\n",
    "save(val_results_KP1,'val_results_KP1_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model3, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KP1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KP1, col_KP1[0]), 'test_results_KP1_MLP')\n",
    "save(test_results_KP1,'test_results_KP1_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model3 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_KP1[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_KP1[0]]\n",
    "\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model3, X_train, window_train, y_train)\n",
    "train_results_KP1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_KP1, col_KP1[0]),'train_results_KP1_LINEAR')\n",
    "save(train_results_KP1,'train_results_KP1_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model3, X_val, window_val, y_val)\n",
    "val_results_KP1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KP1, col_KP1[0]),'val_results_KP1_LINEAR')\n",
    "save(val_results_KP1,'val_results_KP1_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model3, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KP1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KP1, col_KP1[0]), 'test_results_KP1_LINEAR')\n",
    "save(test_results_KP1,'test_results_KP1_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_KP1_MLP'), \"MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KP1_MLP'), get('val_results_KP1_MLP'), get('test_results_KP1_MLP'), \"MODEL 3 MLP ARX - Model pomiaru ciśnienia KP1 w komorze paleniskowej\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_KP1_LINEAR'), \"MODEL 3 - Model pomiaru ciśnienia KP1 w komorze paleniskowej LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KP1_LINEAR'), get('val_results_KP1_LINEAR'), get('test_results_KP1_LINEAR'), \"MODEL 3 LINEAR ARX - Model pomiaru ciśnienia KP1 w komorze paleniskowej\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_KP1_normalized_LSTM\", \"val_results_KP1_normalized_LSTM\", \"test_results_KP1_normalized_LSTM\", \"train_results_KP1_normalized_MLP\", \"val_results_KP1_normalized_MLP\", \"test_results_KP1_normalized_MLP\", \"train_results_KP1_normalized_LINEAR\", \"val_results_KP1_normalized_LINEAR\", \"test_results_KP1_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 3 - Model pomiaru ciśnienia KP1', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/KP1_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/KP1_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_KO1_LSTM'), get('test_results_KO1_MLP'), get('test_results_KO1_LINEAR'), \"MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_KO1 = file_data[col_KO1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KO1[col_KO1[0]], WINDOW_SIZE)\n",
    "  var_train = df_KO1[col_KO1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_KO1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_KO1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model(18) #18 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KO1/model_KO1.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model4.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model4 = load_model('model_KO1/model_KO1.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_KO1[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_KO1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model4, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_KO1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_KO1, col_KO1[0]),'train_results_KO1')\n",
    "# save(train_results_KO1,'train_results_KO1_normalized')\n",
    "\n",
    "# val_predictions = predict(model4, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_KO1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_KO1, col_KO1[0]),'val_results_KO1')\n",
    "# save(val_results_KO1,'val_results_KO1_normalized')\n",
    "\n",
    "test_predictions = predict(model4, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_KO1 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_KO1, col_KO1[0]), 'test_results_KO1')\n",
    "save(test_results_KO1,'test_results_KO1_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KO1'), \"MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła LSTM\")\n",
    "#display(get('train_results_KO1'), get('val_results_KO1'), get('test_results_KO1'),'all', \"MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_KO1 = file_data[col_KO1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KO1[col_KO1[0]], WINDOW_SIZE)\n",
    "  var_train = df_KO1[col_KO1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_KO1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_KO1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KO1 = file_data[col_KO1]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_KO1[col_KO1[0]], WINDOW_SIZE)\n",
    "  var_test = df_KO1[col_KO1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model(18) #18 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KO1/model_KO1_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model4.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model4 = load_model('model_KO1/model_KO1_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model4, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_KO1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KO1, col_KO1[0]), 'train_results_KO1_LSTM')\n",
    "save(train_results_KO1,'train_results_KO1_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model4, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_KO1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KO1, col_KO1[0]),'val_results_KO1_LSTM')\n",
    "save(val_results_KO1,'val_results_KO1_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model4, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_KO1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KO1, col_KO1[0]), 'test_results_KO1_LSTM')\n",
    "save(test_results_KO1,'test_results_KO1_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KO1_LSTM'), \"MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KO1_LSTM'), get('val_results_KO1_LSTM'), get('test_results_KO1_LSTM'), \"MODEL 4 LSTM - Model pomiaru ciśnienia KO1 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_KP1 = file_data[col_KO1]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KP1['y_k_1'] = df_KP1[col_KO1[0]].shift(1)  # y[k-1]\n",
    "  df_KP1['y_k_2'] = df_KP1[col_KO1[0]].shift(2)  # y[k-2]\n",
    "  df_KP1 = df_KP1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_KP1[col_KO1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_KP1[col_KO1[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_KO1[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_KO1[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KO1 = file_data[col_KO1]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KO1['y_k_1'] = df_KO1[col_KO1[0]].shift(1)  # y[k-1]\n",
    "  df_KO1['y_k_2'] = df_KO1[col_KO1[0]].shift(2)  # y[k-2]\n",
    "  df_KO1 = df_KO1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_KO1[col_KO1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_KO1[col_KO1[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model4 = create_model_MLP(20) #20 variables (18+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_KO1/model_KO1_MLP.keras', save_best_only=True)\n",
    "\n",
    "model4.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model4.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model4 = load_model('model_KO1/model_KO1_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model4, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_KO1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KO1, col_KO1[0]), 'train_results_KO1_MLP')\n",
    "save(train_results_KO1,'train_results_KO1_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model4, X_val, window_val, y_val)\n",
    "val_results_KO1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KO1, col_KO1[0]),'val_results_KO1_MLP')\n",
    "save(val_results_KO1,'val_results_KO1_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model4, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KO1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KO1, col_KO1[0]), 'test_results_KO1_MLP')\n",
    "save(test_results_KO1,'test_results_KO1_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model4 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_KO1[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_KO1[0]]\n",
    "\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model4, X_train, window_train, y_train)\n",
    "train_results_KO1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_KO1, col_KO1[0]),'train_results_KO1_LINEAR')\n",
    "save(train_results_KO1,'train_results_KO1_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model4, X_val, window_val, y_val)\n",
    "val_results_KO1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KO1, col_KO1[0]),'val_results_KO1_LINEAR')\n",
    "save(val_results_KO1,'val_results_KO1_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model4, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KO1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KO1, col_KO1[0]), 'test_results_KO1_LINEAR')\n",
    "save(test_results_KO1,'test_results_KO1_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_KO1_MLP'), \"MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KO1_MLP'), get('val_results_KO1_MLP'), get('test_results_KO1_MLP'), \"MODEL 4 MLP ARX - Model pomiaru ciśnienia KO1 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_KO1_LINEAR'), \"MODEL 4 - Model pomiaru ciśnienia KO1 na wyjściu z kotła LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KO1_LINEAR'), get('val_results_KO1_LINEAR'), get('test_results_KO1_LINEAR'), \"MODEL 4 LINEAR ARX - Model pomiaru ciśnienia KO1 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_KO1_normalized_LSTM\", \"val_results_KO1_normalized_LSTM\", \"test_results_KO1_normalized_LSTM\", \"train_results_KO1_normalized_MLP\", \"val_results_KO1_normalized_MLP\", \"test_results_KO1_normalized_MLP\", \"train_results_KO1_normalized_LINEAR\", \"val_results_KO1_normalized_LINEAR\", \"test_results_KO1_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 4 - Model pomiaru ciśnienia KO1', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/KO1_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/KO1_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_KO2_LSTM'), get('test_results_KO2_MLP'), get('test_results_KO2_LINEAR'), \"MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_KO2 = file_data[col_KO2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KO2[col_KO2[0]], WINDOW_SIZE)\n",
    "  var_train = df_KO2[col_KO2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_KO2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_KO2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model5 = create_model(17) #17 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KO2/model_KO2.keras', save_best_only=True)\n",
    "\n",
    "model5.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model5.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model5 = load_model('model_KO2/model_KO2.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_KO2[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_KO2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model5, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_KO2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_KO2, col_KO2[0]),'train_results_KO2')\n",
    "# save(train_results_KO2,'train_results_KO2_normalized')\n",
    "\n",
    "# val_predictions = predict(model5, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_KO2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_KO2, col_KO2[0]),'val_results_KO2')\n",
    "# save(val_results_KO2,'val_results_KO2_normalized')\n",
    "\n",
    "test_predictions = predict(model5, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_KO2 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_KO2, col_KO2[0]), 'test_results_KO2')\n",
    "save(test_results_KO2,'test_results_KO2_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KO2'), \"MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła LSTM\")\n",
    "#display(get('train_results_KO2'), get('val_results_KO2'), get('test_results_KO2'),'all', \"MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_KO2 = file_data[col_KO2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KO2[col_KO2[0]], WINDOW_SIZE)\n",
    "  var_train = df_KO2[col_KO2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_KO2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_KO2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KO2 = file_data[col_KO2]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_KO2[col_KO2[0]], WINDOW_SIZE)\n",
    "  var_test = df_KO2[col_KO2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model5 = create_model(17) #17 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KO2/model_KO2_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model5.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model5.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model5 = load_model('model_KO2/model_KO2_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model5, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_KO2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KO2, col_KO2[0]), 'train_results_KO2_LSTM')\n",
    "save(train_results_KO2,'train_results_KO2_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model5, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_KO2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KO2, col_KO2[0]),'val_results_KO2_LSTM')\n",
    "save(val_results_KO2,'val_results_KO2_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model5, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_KO2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KO2, col_KO2[0]), 'test_results_KO2_LSTM')\n",
    "save(test_results_KO2,'test_results_KO2_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KO2_LSTM'), \"MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KO2_LSTM'), get('val_results_KO2_LSTM'), get('test_results_KO2_LSTM'), \"MODEL 5 LSTM - Model pomiaru ciśnienia KO2 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_KO2 = file_data[col_KO2]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KO2['y_k_1'] = df_KO2[col_KO2[0]].shift(1)  # y[k-1]\n",
    "  df_KO2['y_k_2'] = df_KO2[col_KO2[0]].shift(2)  # y[k-2]\n",
    "  df_KO2 = df_KO2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_KO2[col_KO2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_KO2[col_KO2[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_KO2[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_KO2[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KO2 = file_data[col_KO2]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KO2['y_k_1'] = df_KO2[col_KO2[0]].shift(1)  # y[k-1]\n",
    "  df_KO2['y_k_2'] = df_KO2[col_KO2[0]].shift(2)  # y[k-2]\n",
    "  df_KO2 = df_KO2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_KO2[col_KO2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_KO2[col_KO2[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model5 = create_model_MLP(19) #19 variables (17+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_KO2/model_KO2_MLP.keras', save_best_only=True)\n",
    "\n",
    "model5.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model5.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model5 = load_model('model_KO2/model_KO2_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model5, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_KO2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KO2, col_KO2[0]), 'train_results_KO2_MLP')\n",
    "save(train_results_KO2,'train_results_KO2_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model5, X_val, window_val, y_val)\n",
    "val_results_KO2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KO2, col_KO2[0]),'val_results_KO2_MLP')\n",
    "save(val_results_KO2,'val_results_KO2_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model5, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KO2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KO2, col_KO2[0]), 'test_results_KO2_MLP')\n",
    "save(test_results_KO2,'test_results_KO2_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model5 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_KO2[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_KO2[0]]\n",
    "\n",
    "model5.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model5, X_train, window_train, y_train)\n",
    "train_results_KO2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_KO2, col_KO2[0]),'train_results_KO2_LINEAR')\n",
    "save(train_results_KO2,'train_results_KO2_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model5, X_val, window_val, y_val)\n",
    "val_results_KO2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KO2, col_KO2[0]),'val_results_KO2_LINEAR')\n",
    "save(val_results_KO2,'val_results_KO2_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model5, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KO2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KO2, col_KO2[0]), 'test_results_KO2_LINEAR')\n",
    "save(test_results_KO2,'test_results_KO2_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_KO2_MLP'), \"MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KO2_MLP'), get('val_results_KO2_MLP'), get('test_results_KO2_MLP'), \"MODEL 5 MLP ARX - Model pomiaru ciśnienia KO2 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_KO2_LINEAR'), \"MODEL 5 - Model pomiaru ciśnienia KO2 na wyjściu z kotła LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KO2_LINEAR'), get('val_results_KO2_LINEAR'), get('test_results_KO2_LINEAR'), \"MODEL 5 LINEAR ARX - Model pomiaru ciśnienia KO2 na wyjściu z kotła\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_KO2_normalized_LSTM\", \"val_results_KO2_normalized_LSTM\", \"test_results_KO2_normalized_LSTM\", \"train_results_KO2_normalized_MLP\", \"val_results_KO2_normalized_MLP\", \"test_results_KO2_normalized_MLP\", \"train_results_KO2_normalized_LINEAR\", \"val_results_KO2_normalized_LINEAR\", \"test_results_KO2_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 5 - Model pomiaru ciśnienia KO2', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/KO2_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/KO2_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 6 - Model pomiaru ciśnienia spalin KS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_KS1_LSTM'), get('test_results_KS1_MLP'), get('test_results_KS1_LINEAR'), \"MODEL 6 - Model pomiaru ciśnienia spalin KS1\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_KS1 = file_data[col_KS1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KS1[col_KS1[0]], WINDOW_SIZE)\n",
    "  var_train = df_KS1[col_KS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_KS1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_KS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model6 = create_model(4) #4 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KS1/model_KS1.keras', save_best_only=True)\n",
    "\n",
    "model6.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model6.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model6 = load_model('model_KS1/model_KS1.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_KS1[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_KS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model6, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_KS1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_KS1, col_KS1[0]),'train_results_KS1')\n",
    "# save(train_results_KS1,'train_results_KS1_normalized')\n",
    "\n",
    "# val_predictions = predict(model6, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_KS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_KS1, col_KS1[0]),'val_results_KS1')\n",
    "# save(val_results_KS1,'val_results_KS1_normalized')\n",
    "\n",
    "test_predictions = predict(model6, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_KS1 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_KS1, col_KS1[0]), 'test_results_KS1')\n",
    "save(test_results_KS1,'test_results_KS1_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KS1'), \"MODEL 6 - Model pomiaru ciśnienia spalin KS1 LSTM\")\n",
    "#display(get('train_results_KS1'), get('val_results_KS1'), get('test_results_KS1'),'all', \"MODEL 6 - Model pomiaru ciśnienia spalin KS1 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_KS1 = file_data[col_KS1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KS1[col_KS1[0]], WINDOW_SIZE)\n",
    "  var_train = df_KS1[col_KS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_KS1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_KS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KS1 = file_data[col_KS1]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_KS1[col_KS1[0]], WINDOW_SIZE)\n",
    "  var_test = df_KS1[col_KS1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model6 = create_model(4) #4 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KS1/model_KS1_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model6.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model6.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model6 = load_model('model_KS1/model_KS1_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model6, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_KS1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KS1, col_KS1[0]), 'train_results_KS1_LSTM')\n",
    "save(train_results_KS1,'train_results_KS1_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model6, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_KS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KS1, col_KS1[0]),'val_results_KS1_LSTM')\n",
    "save(val_results_KS1,'val_results_KS1_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model6, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_KS1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KS1, col_KS1[0]), 'test_results_KS1_LSTM')\n",
    "save(test_results_KS1,'test_results_KS1_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KS1_LSTM'), \"MODEL 6 - Model pomiaru ciśnienia spalin KS1 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KS1_LSTM'), get('val_results_KS1_LSTM'), get('test_results_KS1_LSTM'), \"MODEL 6 LSTM - Model pomiaru ciśnienia spalin KS1\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_KS1 = file_data[col_KS1]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KS1['y_k_1'] = df_KS1[col_KS1[0]].shift(1)  # y[k-1]\n",
    "  df_KS1['y_k_2'] = df_KS1[col_KS1[0]].shift(2)  # y[k-2]\n",
    "  df_KS1 = df_KS1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_KS1[col_KS1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_KS1[col_KS1[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_KS1[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_KS1[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KS1 = file_data[col_KS1]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KS1['y_k_1'] = df_KS1[col_KS1[0]].shift(1)  # y[k-1]\n",
    "  df_KS1['y_k_2'] = df_KS1[col_KS1[0]].shift(2)  # y[k-2]\n",
    "  df_KS1 = df_KS1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_KS1[col_KS1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_KS1[col_KS1[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model6 = create_model_MLP(6) #6 variables (4+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_KS1/model_KS1_MLP.keras', save_best_only=True)\n",
    "\n",
    "model6.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model6.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model6 = load_model('model_KS1/model_KS1_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model6, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_KS1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KS1, col_KS1[0]), 'train_results_KS1_MLP')\n",
    "save(train_results_KS1,'train_results_KS1_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model6, X_val, window_val, y_val)\n",
    "val_results_KS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KS1, col_KS1[0]),'val_results_KS1_MLP')\n",
    "save(val_results_KS1,'val_results_KS1_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model6, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KS1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KS1, col_KS1[0]), 'test_results_KS1_MLP')\n",
    "save(test_results_KS1,'test_results_KS1_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model6 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_KS1[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_KS1[0]]\n",
    "\n",
    "model6.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model6, X_train, window_train, y_train)\n",
    "train_results_KS1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_KS1, col_KS1[0]),'train_results_KS1_LINEAR')\n",
    "save(train_results_KS1,'train_results_KS1_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model6, X_val, window_val, y_val)\n",
    "val_results_KS1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KS1, col_KS1[0]),'val_results_KS1_LINEAR')\n",
    "save(val_results_KS1,'val_results_KS1_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model6, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KS1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KS1, col_KS1[0]), 'test_results_KS1_LINEAR')\n",
    "save(test_results_KS1,'test_results_KS1_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_KS1_MLP'), \"MODEL 6 - Model pomiaru ciśnienia spalin KS1 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KS1_MLP'), get('val_results_KS1_MLP'), get('test_results_KS1_MLP'), \"MODEL 6 MLP ARX - Model pomiaru ciśnienia spalin KS1\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_KS1_LINEAR'), \"MODEL 6 - Model pomiaru ciśnienia spalin KS1 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KS1_LINEAR'), get('val_results_KS1_LINEAR'), get('test_results_KS1_LINEAR'), \"MODEL 6 LINEAR ARX - Model pomiaru ciśnienia spalin KS1\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_KS1_normalized_LSTM\", \"val_results_KS1_normalized_LSTM\", \"test_results_KS1_normalized_LSTM\", \"train_results_KS1_normalized_MLP\", \"val_results_KS1_normalized_MLP\", \"test_results_KS1_normalized_MLP\", \"train_results_KS1_normalized_LINEAR\", \"val_results_KS1_normalized_LINEAR\", \"test_results_KS1_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 6 - Model pomiaru ciśnienia spalin KS1', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/KS1_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/KS1_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 7 - Model pomiaru ciśnienia spalin KS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_KS2_LSTM'), get('test_results_KS2_MLP'), get('test_results_KS2_LINEAR'), \"MODEL 7 - Model pomiaru ciśnienia spalin KS2\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_KS2 = file_data[col_KS2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KS2[col_KS2[0]], WINDOW_SIZE)\n",
    "  var_train = df_KS2[col_KS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_KS2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_KS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model7 = create_model(5) #5 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KS2/model_KS2.keras', save_best_only=True)\n",
    "\n",
    "model7.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model7.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model7 = load_model('model_KS2/model_KS2.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_KS2[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_KS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model7, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_KS2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_KS2, col_KS2[0]),'train_results_KS2')\n",
    "# save(train_results_KS2,'train_results_KS2_normalized')\n",
    "\n",
    "# val_predictions = predict(model7, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_KS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_KS2, col_KS2[0]),'val_results_KS2')\n",
    "# save(val_results_KS2,'val_results_KS2_normalized')\n",
    "\n",
    "test_predictions = predict(model7, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_KS2 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_KS2, col_KS2[0]), 'test_results_KS2')\n",
    "save(test_results_KS2,'test_results_KS2_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KS2'), \"MODEL 7 - Model pomiaru ciśnienia spalin KS2 LSTM\")\n",
    "#display(get('train_results_KS2'), get('val_results_KS2'), get('test_results_KS2'),'all', \"MODEL 7 - Model pomiaru ciśnienia spalin KS2 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_KS2 = file_data[col_KS2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_KS2[col_KS2[0]], WINDOW_SIZE)\n",
    "  var_train = df_KS2[col_KS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_KS2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_KS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KS2 = file_data[col_KS2]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_KS2[col_KS2[0]], WINDOW_SIZE)\n",
    "  var_test = df_KS2[col_KS2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model7 = create_model(5) #5 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_KS2/model_KS2_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model7.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model7.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model7 = load_model('model_KS2/model_KS2_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model7, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_KS2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KS2, col_KS2[0]), 'train_results_KS2_LSTM')\n",
    "save(train_results_KS2,'train_results_KS2_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model7, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_KS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KS2, col_KS2[0]),'val_results_KS2_LSTM')\n",
    "save(val_results_KS2,'val_results_KS2_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model7, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_KS2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KS2, col_KS2[0]), 'test_results_KS2_LSTM')\n",
    "save(test_results_KS2,'test_results_KS2_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_KS2_LSTM'), \"MODEL 7 - Model pomiaru ciśnienia spalin KS2 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KS2_LSTM'), get('val_results_KS2_LSTM'), get('test_results_KS2_LSTM'), \"MODEL 7 LSTM - Model pomiaru ciśnienia spalin KS2\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_KS2 = file_data[col_KS2]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KS2['y_k_1'] = df_KS2[col_KS2[0]].shift(1)  # y[k-1]\n",
    "  df_KS2['y_k_2'] = df_KS2[col_KS2[0]].shift(2)  # y[k-2]\n",
    "  df_KS2 = df_KS2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_KS2[col_KS2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_KS2[col_KS2[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_KS2[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_KS2[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_KS2 = file_data[col_KS2]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_KS2['y_k_1'] = df_KS2[col_KS2[0]].shift(1)  # y[k-1]\n",
    "  df_KS2['y_k_2'] = df_KS2[col_KS2[0]].shift(2)  # y[k-2]\n",
    "  df_KS2 = df_KS2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_KS2[col_KS2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_KS2[col_KS2[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model7 = create_model_MLP(7) #7 variables (5+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_KS2/model_KS2_MLP.keras', save_best_only=True)\n",
    "\n",
    "model7.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model7.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model7 = load_model('model_KS2/model_KS2_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model7, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_KS2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_KS2, col_KS2[0]), 'train_results_KS2_MLP')\n",
    "save(train_results_KS2,'train_results_KS2_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model7, X_val, window_val, y_val)\n",
    "val_results_KS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KS2, col_KS2[0]),'val_results_KS2_MLP')\n",
    "save(val_results_KS2,'val_results_KS2_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model7, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KS2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KS2, col_KS2[0]), 'test_results_KS2_MLP')\n",
    "save(test_results_KS2,'test_results_KS2_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model7 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_KS2[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_KS2[0]]\n",
    "\n",
    "model7.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model7, X_train, window_train, y_train)\n",
    "train_results_KS2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_KS2, col_KS2[0]),'train_results_KS2_LINEAR')\n",
    "save(train_results_KS2,'train_results_KS2_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model7, X_val, window_val, y_val)\n",
    "val_results_KS2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_KS2, col_KS2[0]),'val_results_KS2_LINEAR')\n",
    "save(val_results_KS2,'val_results_KS2_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model7, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_KS2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_KS2, col_KS2[0]), 'test_results_KS2_LINEAR')\n",
    "save(test_results_KS2,'test_results_KS2_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_KS2_MLP'), \"MODEL 7 - Model pomiaru ciśnienia spalin KS2 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KS2_MLP'), get('val_results_KS2_MLP'), get('test_results_KS2_MLP'), \"MODEL 7 MLP ARX - Model pomiaru ciśnienia spalin KS2\", y_unit='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_KS2_LINEAR'), \"MODEL 7 - Model pomiaru ciśnienia spalin KS2 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_KS2_LINEAR'), get('val_results_KS2_LINEAR'), get('test_results_KS2_LINEAR'), \"MODEL 7 LINEAR ARX - Model pomiaru ciśnienia spalin KS2\", y_unit='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_KS2_normalized_LSTM\", \"val_results_KS2_normalized_LSTM\", \"test_results_KS2_normalized_LSTM\", \"train_results_KS2_normalized_MLP\", \"val_results_KS2_normalized_MLP\", \"test_results_KS2_normalized_MLP\", \"train_results_KS2_normalized_LINEAR\", \"val_results_KS2_normalized_LINEAR\", \"test_results_KS2_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 7 - Model pomiaru ciśnienia spalin KS2', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/KS2_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/KS2_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_O1_LSTM'), get('test_results_O1_MLP'), get('test_results_O1_LINEAR'), \"MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1\", y_unit = '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_O1 = file_data[col_O1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_O1[col_O1[0]], WINDOW_SIZE)\n",
    "  var_train = df_O1[col_O1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_O1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_O1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model8 = create_model(16) #16 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_O1/model_O1.keras', save_best_only=True)\n",
    "\n",
    "model8.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model8.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model8 = load_model('model_O1/model_O1.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_O1[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_O1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model8, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_01 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_01, col_O1[0]),'train_results_01')\n",
    "# save(train_results_01,'train_results_O1_normalized')\n",
    "\n",
    "# val_predictions = predict(model8, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_O1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_O1, col_O1[0]),'val_results_O1')\n",
    "# save(val_results_O1,'val_results_O1_normalized')\n",
    "\n",
    "test_predictions = predict(model8, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_O1 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_O1, col_O1[0]), 'test_results_O1')\n",
    "save(test_results_O1,'test_results_O1_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_O1'), \"MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1 LSTM\")\n",
    "#display(get('train_results_O1'), get('val_results_O1'), get('test_results_O1'),'all', \"MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_O1 = file_data[col_O1]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_O1[col_O1[0]], WINDOW_SIZE)\n",
    "  var_train = df_O1[col_O1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_O1[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_O1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_O1 = file_data[col_O1]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_O1[col_O1[0]], WINDOW_SIZE)\n",
    "  var_test = df_O1[col_O1[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model8 = create_model(16) #16 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_O1/model_O1_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model8.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model8.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model8 = load_model('model_O1/model_O1_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model8, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_O1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_O1, col_O1[0]), 'train_results_O1_LSTM')\n",
    "save(train_results_O1,'train_results_O1_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model8, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_O1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O1, col_O1[0]),'val_results_O1_LSTM')\n",
    "save(val_results_O1,'val_results_O1_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model8, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_O1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O1, col_O1[0]), 'test_results_O1_LSTM')\n",
    "save(test_results_O1,'test_results_O1_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_O1_LSTM'), \"MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O1_LSTM'), get('val_results_O1_LSTM'), get('test_results_O1_LSTM'), \"MODEL 8 LSTM - Model pomiaru zawartości tlenu w spalinach O1\", y_unit='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_O1 = file_data[col_O1]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_O1['y_k_1'] = df_O1[col_O1[0]].shift(1)  # y[k-1]\n",
    "  df_O1['y_k_2'] = df_O1[col_O1[0]].shift(2)  # y[k-2]\n",
    "  df_O1 = df_O1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_O1[col_O1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_O1[col_O1[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_O1[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_O1[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_O1 = file_data[col_O1]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_O1['y_k_1'] = df_O1[col_O1[0]].shift(1)  # y[k-1]\n",
    "  df_O1['y_k_2'] = df_O1[col_O1[0]].shift(2)  # y[k-2]\n",
    "  df_O1 = df_O1.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_O1[col_O1[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_O1[col_O1[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model8 = create_model_MLP(18) #18 variables (16+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_O1/model_O1_MLP.keras', save_best_only=True)\n",
    "\n",
    "model8.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model8.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model8 = load_model('model_O1/model_O1_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model8, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_O1 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_O1, col_O1[0]), 'train_results_O1_MLP')\n",
    "save(train_results_O1,'train_results_O1_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model8, X_val, window_val, y_val)\n",
    "val_results_O1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O1, col_O1[0]),'val_results_O1_MLP')\n",
    "save(val_results_O1,'val_results_O1_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model8, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_O1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O1, col_O1[0]), 'test_results_O1_MLP')\n",
    "save(test_results_O1,'test_results_O1_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model8 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_O1[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_O1[0]]\n",
    "\n",
    "model8.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model8, X_train, window_train, y_train)\n",
    "train_results_O1 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_O1, col_O1[0]),'train_results_O1_LINEAR')\n",
    "save(train_results_O1,'train_results_O1_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model8, X_val, window_val, y_val)\n",
    "val_results_O1 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O1, col_O1[0]),'val_results_O1_LINEAR')\n",
    "save(val_results_O1,'val_results_O1_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model8, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_O1 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O1, col_O1[0]), 'test_results_O1_LINEAR')\n",
    "save(test_results_O1,'test_results_O1_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_O1_MLP'), \"MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O1_MLP'), get('val_results_O1_MLP'), get('test_results_O1_MLP'), \"MODEL 8 MLP ARX - Model pomiaru zawartości tlenu w spalinach O1\", y_unit='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_O1_LINEAR'), \"MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O1_LINEAR'), get('val_results_O1_LINEAR'), get('test_results_O1_LINEAR'), \"MODEL 8 LINEAR ARX - Model pomiaru zawartości tlenu w spalinach O1\", y_unit='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_O1_normalized_LSTM\", \"val_results_O1_normalized_LSTM\", \"test_results_O1_normalized_LSTM\", \"train_results_O1_normalized_MLP\", \"val_results_O1_normalized_MLP\", \"test_results_O1_normalized_MLP\", \"train_results_O1_normalized_LINEAR\", \"val_results_O1_normalized_LINEAR\", \"test_results_O1_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 8 - Model pomiaru zawartości tlenu w spalinach O1', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/O1_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/O1_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_O2_LSTM'), get('test_results_O2_MLP'), get('test_results_O2_LINEAR'), \"MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2\", y_unit = '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_O2 = file_data[col_O2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_O2[col_O2[0]], WINDOW_SIZE)\n",
    "  var_train = df_O2[col_O2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_O2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_O2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model9 = create_model(6) #6 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_O2/model_O2.keras', save_best_only=True)\n",
    "\n",
    "model9.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model9.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model9 = load_model('model_O2/model_O2.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_O2[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_O2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model9, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_O2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_O2, col_O2[0]),'train_results_O2')\n",
    "# save(train_results_O2,'train_results_O2_normalized')\n",
    "\n",
    "# val_predictions = predict(model9, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_O2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_O2, col_O2[0]),'val_results_O2')\n",
    "# save(val_results_O2,'val_results_O2_normalized')\n",
    "\n",
    "test_predictions = predict(model9, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_O2 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_O2, col_O2[0]), 'test_results_O2')\n",
    "save(test_results_O2,'test_results_O2_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_O2'), \"MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2 LSTM\")\n",
    "#display(get('train_results_O2'), get('val_results_O2'), get('test_results_O2'),'all', \"MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_O2 = file_data[col_O2]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_O2[col_O2[0]], WINDOW_SIZE)\n",
    "  var_train = df_O2[col_O2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_O2[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_O2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_O2 = file_data[col_O2]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_O2[col_O2[0]], WINDOW_SIZE)\n",
    "  var_test = df_O2[col_O2[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model9 = create_model(6) #6 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_O2/model_O2_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model9.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model9.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model9 = load_model('model_O2/model_O2_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model9, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_O2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_O2, col_O2[0]), 'train_results_O2_LSTM')\n",
    "save(train_results_O2,'train_results_O2_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model9, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_O2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O2, col_O2[0]),'val_results_O2_LSTM')\n",
    "save(val_results_O2,'val_results_O2_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model9, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_O2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O2, col_O2[0]), 'test_results_O2_LSTM')\n",
    "save(test_results_O2,'test_results_O2_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_O2_LSTM'), \"MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O2_LSTM'), get('val_results_O2_LSTM'), get('test_results_O2_LSTM'), \"MODEL 9 LSTM - Model pomiaru zawartości tlenu w spalinach O2\", y_unit='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_O2 = file_data[col_O2]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_O2['y_k_1'] = df_O2[col_O2[0]].shift(1)  # y[k-1]\n",
    "  df_O2['y_k_2'] = df_O2[col_O2[0]].shift(2)  # y[k-2]\n",
    "  df_O2 = df_O2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_O2[col_O2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_O2[col_O2[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_O2[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_O2[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_O2 = file_data[col_O2]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_O2['y_k_1'] = df_O2[col_O2[0]].shift(1)  # y[k-1]\n",
    "  df_O2['y_k_2'] = df_O2[col_O2[0]].shift(2)  # y[k-2]\n",
    "  df_O2 = df_O2.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_O2[col_O2[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_O2[col_O2[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model9 = create_model_MLP(8) #8 variables (6+2y) \n",
    "\n",
    "cp = ModelCheckpoint('model_O2/model_O2_MLP.keras', save_best_only=True)\n",
    "\n",
    "model9.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model9.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model9 = load_model('model_O2/model_O2_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model9, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_O2 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_O2, col_O2[0]), 'train_results_O2_MLP')\n",
    "save(train_results_O2,'train_results_O2_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model9, X_val, window_val, y_val)\n",
    "val_results_O2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O2, col_O2[0]),'val_results_O2_MLP')\n",
    "save(val_results_O2,'val_results_O2_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model9, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_O2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O2, col_O2[0]), 'test_results_O2_MLP')\n",
    "save(test_results_O2,'test_results_O2_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model9 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_O2[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_O2[0]]\n",
    "\n",
    "model9.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model9, X_train, window_train, y_train)\n",
    "train_results_O2 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_O2, col_O2[0]),'train_results_O2_LINEAR')\n",
    "save(train_results_O2,'train_results_O2_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model9, X_val, window_val, y_val)\n",
    "val_results_O2 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O2, col_O2[0]),'val_results_O2_LINEAR')\n",
    "save(val_results_O2,'val_results_O2_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model9, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_O2 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O2, col_O2[0]), 'test_results_O2_LINEAR')\n",
    "save(test_results_O2,'test_results_O2_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_O2_MLP'), \"MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O2_MLP'), get('val_results_O2_MLP'), get('test_results_O2_MLP'), \"MODEL 9 MLP ARX - Model pomiaru zawartości tlenu w spalinach O2\", y_unit='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_O2_LINEAR'), \"MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O2_LINEAR'), get('val_results_O2_LINEAR'), get('test_results_O2_LINEAR'), \"MODEL 9 LINEAR ARX - Model pomiaru zawartości tlenu w spalinach O2\", y_unit='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_O2_normalized_LSTM\", \"val_results_O2_normalized_LSTM\", \"test_results_O2_normalized_LSTM\", \"train_results_O2_normalized_MLP\", \"val_results_O2_normalized_MLP\", \"test_results_O2_normalized_MLP\", \"train_results_O2_normalized_LINEAR\", \"val_results_O2_normalized_LINEAR\", \"test_results_O2_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 9 - Model pomiaru zawartości tlenu w spalinach O2', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/O2_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/O2_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison\n",
    "display_comparison(get('test_results_O3_LSTM'), get('test_results_O3_MLP'), get('test_results_O3_LINEAR'), \"MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3\", y_unit = '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  df_O3 = file_data[col_O3]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_O3[col_O3[0]], WINDOW_SIZE)\n",
    "  var_train = df_O3[col_O3[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data[col_O3[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data[col_O3[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model10 = create_model(12) #12 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_O3/model_O3.keras', save_best_only=True)\n",
    "\n",
    "model10.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model10.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions\n",
    "model10 = load_model('model_O3/model_O3.keras')\n",
    "\n",
    "X_test, y_test = data_to_X_y(test_file_data[col_O3[0]], WINDOW_SIZE)\n",
    "var_test = test_file_data[col_O3[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "\n",
    "test = 0\n",
    "window_test = X_test.shape[0]\n",
    "# train = 0\n",
    "# window_train = X_train.shape[0]\n",
    "# val = 0\n",
    "# window_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "# train_predictions = predict(model10, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train)\n",
    "# train_results_O3 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "# save(denorm(train_results_O3, col_O3[0]),'train_results_O3')\n",
    "# save(train_results_O3,'train_results_O3_normalized')\n",
    "\n",
    "# val_predictions = predict(model10, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val)\n",
    "# val_results_O3 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "# save(denorm(val_results_O3, col_O3[0]),'val_results_O3')\n",
    "# save(val_results_O3,'val_results_O3_normalized')\n",
    "\n",
    "test_predictions = predict(model10, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test)\n",
    "test_results_O3 = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "save(denorm(test_results_O3, col_O3[0]), 'test_results_O3')\n",
    "save(test_results_O3,'test_results_O3_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_O3'), \"MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3 LSTM\")\n",
    "#display(get('train_results_O3'), get('val_results_O3'), get('test_results_O3'),'all', \"MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3 LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "prepared_data = []\n",
    "WINDOW_SIZE = 10\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "   #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  df_O3 = file_data[col_O3]\n",
    "  \n",
    "  X_train, y_train = data_to_X_y(df_O3[col_O3[0]], WINDOW_SIZE)\n",
    "  var_train = df_O3[col_O3[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_train = min(X_train.shape[0],min(y_train.shape[0], var_train.shape[0]))\n",
    "  X_train = X_train[:length_train] \n",
    "  y_train = y_train[:length_train]\n",
    "  var_train = var_train[:length_train] \n",
    "  \n",
    "  prepared_data.append([X_train, y_train, var_train])\n",
    "\n",
    "X_val, y_val = data_to_X_y(val_file_data_resampled[col_O3[0]], WINDOW_SIZE)\n",
    "var_val = val_file_data_resampled[col_O3[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "\n",
    "length_val = min(X_val.shape[0],min(y_val.shape[0], var_val.shape[0]))\n",
    "X_val = X_val[:length_val] \n",
    "y_val = y_val[:length_val] \n",
    "var_val = var_val[:length_val]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_O3 = file_data[col_O3]\n",
    "  \n",
    "  X_test, y_test = data_to_X_y(df_O3[col_O3[0]], WINDOW_SIZE)\n",
    "  var_test = df_O3[col_O3[1:]][WINDOW_SIZE-1:].to_numpy()\n",
    "  \n",
    "  length_test = min(X_test.shape[0],min(y_test.shape[0], var_test.shape[0]))\n",
    "  X_test = X_test[:length_test] \n",
    "  y_test = y_test[:length_test] \n",
    "  var_test = var_test[:length_test] \n",
    "  \n",
    "  test_data.append([X_test, y_test, var_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model10 = create_model(12) #12 additional variables not including sequences\n",
    "\n",
    "cp = ModelCheckpoint('model_O3/model_O3_LSTM.keras', save_best_only=True)\n",
    "\n",
    "model10.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model10.fit([X_train, var_train], y_train, validation_data=([X_val, var_val], y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model10 = load_model('model_O3/model_O3_LSTM.keras')\n",
    "train_results = []\n",
    "for [X_train, y_train, var_train] in prepared_data:\n",
    "  #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict(model10, X_train, var_train[train:train+window_train], WINDOW_SIZE, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "\n",
    "train_results_O3 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_O3, col_O3[0]), 'train_results_O3_LSTM')\n",
    "save(train_results_O3,'train_results_O3_normalized_LSTM')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict(model10, X_val, var_val[val:val+window_val], WINDOW_SIZE, window_val, y_val)\n",
    "val_results_O3 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O3, col_O3[0]),'val_results_O3_LSTM')\n",
    "save(val_results_O3,'val_results_O3_normalized_LSTM')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test, var_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict(model10, X_test, var_test[test:test+window_test], WINDOW_SIZE, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "\n",
    "test_results_O3 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O3, col_O3[0]), 'test_results_O3_LSTM')\n",
    "save(test_results_O3,'test_results_O3_normalized_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display_test_results(get('test_results_O3_LSTM'), \"MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3 LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O3_LSTM'), get('val_results_O3_LSTM'), get('test_results_O3_LSTM'), \"MODEL 10 LSTM - Model pomiaru zawartości tlenu w spalinach O3\", y_unit='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ARX and LINEAR ARX resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Przetrwarzamy dane tak, żeby można było przekazać dwa poprzednie kroki zmiennej jako parametry do naszego modelu\n",
    "\n",
    "prepared_data = []\n",
    "for file in train_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True) \n",
    "  \n",
    "  #Taking only the columns relevant for the model\n",
    "  df_O3 = file_data[col_O3]\n",
    "  \n",
    "  # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_O3['y_k_1'] = df_O3[col_O3[0]].shift(1)  # y[k-1]\n",
    "  df_O3['y_k_2'] = df_O3[col_O3[0]].shift(2)  # y[k-2]\n",
    "  df_O3 = df_O3.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_train = df_O3[col_O3[1:] + ['y_k_1','y_k_2']]\n",
    "  y_train = df_O3[col_O3[0]]\n",
    "  \n",
    "  prepared_data.append([X_train, y_train])\n",
    "  \n",
    "X_val = val_file_data_MLP[col_O3[1:] + ['y_k_1','y_k_2']]\n",
    "y_val = val_file_data_MLP[col_O3[0]]\n",
    "\n",
    "test_data = []\n",
    "for file in test_file_paths:\n",
    "  file_data = norm(pd.read_csv(file, delimiter=';')) #normalizujemy dane z przekazanego pliku\n",
    "  file_data = file_data.iloc[2:].reset_index(drop=True) #usuwamy dwa pierwsze wiersze i resetujemy index\n",
    "  \n",
    "  #Resampling the data -> taking every 10th row\n",
    "  file_data = file_data.iloc[::10]\n",
    "  file_data.reset_index(inplace= True, drop= True)\n",
    "  \n",
    "  df_O3 = file_data[col_O3]\n",
    "  \n",
    "    # Creating lagged versions of the target variable (y[k-1], y[k-2])\n",
    "  df_O3['y_k_1'] = df_O3[col_O3[0]].shift(1)  # y[k-1]\n",
    "  df_O3['y_k_2'] = df_O3[col_O3[0]].shift(2)  # y[k-2]\n",
    "  df_O3 = df_O3.dropna().reset_index(drop=True) #Removing the first two rows where y[k-2] or y[k-1] would be NaN\n",
    "  \n",
    "  X_test = df_O3[col_O3[1:] + ['y_k_1','y_k_2']]\n",
    "  y_test = df_O3[col_O3[0]]\n",
    "  \n",
    "  test_data.append([X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "model10 = create_model_MLP(14) #14 variables (12+2y)\n",
    "\n",
    "cp = ModelCheckpoint('model_O3/model_O3_MLP.keras', save_best_only=True)\n",
    "\n",
    "model10.compile(loss = MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "count = 1\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  print(\"\\n\", \"File \",  count, \"/\", len(train_file_paths), \"\\n\")\n",
    "  model10.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks = [cp])\n",
    "  count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model making predictions\n",
    "model10 = load_model('model_O3/model_O3_MLP.keras')\n",
    "\n",
    "train_results = []\n",
    "for [X_train, y_train] in prepared_data:\n",
    "  # #We specify from which point we want to predict the next values for each dataset [s]\n",
    "  # #We specify the predict horizon from the starting point [s]\n",
    "  train = 0\n",
    "  window_train = X_train.shape[0]\n",
    "  train_predictions = predict_MLP(model10, X_train, window_train, y_train)\n",
    "  partial_results = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "  train_results.append(partial_results)\n",
    "  \n",
    "train_results_O3 = pd.concat(train_results, ignore_index=True)\n",
    "save(denorm(train_results_O3, col_O3[0]), 'train_results_O3_MLP')\n",
    "save(train_results_O3,'train_results_O3_normalized_MLP')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_MLP(model10, X_val, window_val, y_val)\n",
    "val_results_O3 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O3, col_O3[0]),'val_results_O3_MLP')\n",
    "save(val_results_O3,'val_results_O3_normalized_MLP')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_MLP(model10, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_O3 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O3, col_O3[0]), 'test_results_O3_MLP')\n",
    "save(test_results_O3,'test_results_O3_normalized_MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "#Model creation linear\n",
    "model10 = LinearRegression()\n",
    "#Model training linear\n",
    "X_train = train_file_data_MLP[col_O3[1:] + ['y_k_1','y_k_2']]\n",
    "y_train = train_file_data_MLP[col_O3[0]]\n",
    "\n",
    "model10.fit(X_train, y_train)\n",
    "\n",
    "#We specify from which point we want to predict the next values for each dataset [s]\n",
    "#We specify the predict horizon from the starting point [s]\n",
    "train = 0\n",
    "window_train = X_train.shape[0]\n",
    "train_predictions = predict_LINEAR(model10, X_train, window_train, y_train)\n",
    "train_results_O3 = pd.DataFrame(data = {'Train Predictions':np.squeeze(train_predictions), 'Actuals':y_train[train:train+window_train]})\n",
    "save(denorm(train_results_O3, col_O3[0]),'train_results_O3_LINEAR')\n",
    "save(train_results_O3,'train_results_O3_normalized_LINEAR')\n",
    "\n",
    "val = 0\n",
    "window_val = X_val.shape[0]\n",
    "val_predictions = predict_LINEAR(model10, X_val, window_val, y_val)\n",
    "val_results_O3 = pd.DataFrame(data = {'Val Predictions':np.squeeze(val_predictions), 'Actuals':y_val[val:val+window_val]})\n",
    "save(denorm(val_results_O3, col_O3[0]),'val_results_O3_LINEAR')\n",
    "save(val_results_O3,'val_results_O3_normalized_LINEAR')\n",
    "\n",
    "test_results = []\n",
    "for [X_test, y_test] in test_data:\n",
    "  test = 0\n",
    "  window_test = X_test.shape[0]\n",
    "  test_predictions = predict_LINEAR(model10, X_test, window_test, y_test)\n",
    "  partial_results = pd.DataFrame(data = {'Test Predictions':np.squeeze(test_predictions), 'Actuals':y_test[test:test+window_test]})\n",
    "  test_results.append(partial_results)\n",
    "  \n",
    "test_results_O3 = pd.concat(test_results, ignore_index=True)\n",
    "save(denorm(test_results_O3, col_O3[0]), 'test_results_O3_LINEAR')\n",
    "save(test_results_O3,'test_results_O3_normalized_LINEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results MLP\n",
    "display_test_results(get('test_results_O3_MLP'), \"MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3 MLP ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O3_MLP'), get('val_results_O3_MLP'), get('test_results_O3_MLP'), \"MODEL 10 MLP ARX - Model pomiaru zawartości tlenu w spalinach O3\", y_unit='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results LINEAR\n",
    "display_test_results(get('test_results_O3_LINEAR'), \"MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3 LINEAR ARX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_train_val_test(get('train_results_O3_LINEAR'), get('val_results_O3_LINEAR'), get('test_results_O3_LINEAR'), \"MODEL 10 LINEAR ARX- Model pomiaru zawartości tlenu w spalinach O3\", y_unit='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "files = [\"train_results_O3_normalized_LSTM\", \"val_results_O3_normalized_LSTM\", \"test_results_O3_normalized_LSTM\", \"train_results_O3_normalized_MLP\", \"val_results_O3_normalized_MLP\", \"test_results_O3_normalized_MLP\", \"train_results_O3_normalized_LINEAR\", \"val_results_O3_normalized_LINEAR\", \"test_results_O3_normalized_LINEAR\"]\n",
    "for file in files:\n",
    "  results = get(file).drop(columns=[\"Unnamed: 0\"])\n",
    "  me, mse, mae, rmse = calculate_metrics(results[results.columns[1]], results[results.columns[0]])#Actuals and then predicitons\n",
    "  metrics.append({'Model': 'model', 'ME': me, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Model'] = ['LSTM train', 'LSTM val', 'LSTM test', 'MLP ARX train', 'MLP ARX val', 'MLP ARX test', 'LINEAR ARX train', 'LINEAR ARX val', 'LINEAR ARX test']\n",
    "metrics_df.rename(columns={'Model': 'MODEL 10 - Model pomiaru zawartości tlenu w spalinach O3', 'ME': 'Mean_Error', 'MSE': 'Mean_Squared_Error', \n",
    "                           'MAE': 'Mean_Absolute_Error', 'RMSE': 'Root_Mean_Squared_Error'}, inplace=True)\n",
    "metrics_df.to_csv(\"metrics/O3_models_metrics_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv(\"metrics/O3_models_metrics_NEW.csv\")\n",
    "metrics_df.iloc[6:, [0, 2, 4]].style.apply(highlight_min_green, subset=metrics_df.iloc[:, [ 2, 4]].columns)#['Mean_Squared_Error', 'Root_Mean_Squared_Error'], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
